{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text_classifier.ipynb","provenance":[{"file_id":"1-GLcGn8_wvQsyMD6VCOHgtZ_Zsw3eqsr","timestamp":1589599796682}],"private_outputs":true,"collapsed_sections":["L3mTvrTLECOP"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"64viHxknCk30"},"source":["# 0. Introduction"]},{"cell_type":"markdown","metadata":{"id":"Ygny9kadCq_w"},"source":["<h1>RUAK - Are you a Hegel?</h1>\n","\n","> The greatest challenge to any thinker is stating the problem in a way that will allow a solution.\n","\n","[Bertrand Russell](https://en.wikipedia.org/wiki/Bertrand_Russell)\n","<br><br>\n","\n","<h2>About the project</h2>\n","Philosophy is a fundamental human thought movement. Everyone is a philosopher. The only question is what kind of philosopher you are. This project tries to answer that question. Using natural language processing (NLP), texts of different authors are used for categorization. With the help of these texts any sentence can be categorically determined.\n","<br>\n","You can open this Jupyter notebook in Google Colab to use a GPU and have a nice platform for editing.\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/stoffy/RUAK-text-classifier/blob/master/notebooks/text_classifier.ipynb)\n","\n","<br>\n","\n","<h2>Information on use</h2>\n","<h3>Paths:</h3>\n","\n","The following data needs to be loaded. Please adjust the paths accordingly ([1.2.1](#1-2-1)): \n","* `source_path` - Path which contains the text files\n","* `dataframe_file_path` - Path for loading and saving the DataFrame\n","* `word2vec_path` - Path to Word2Vec model\n","* `hyperband_tuner_output_path` - Path to hyperparameter tuner working directory \n","* `checkpoint_path` - Path where the checkpoints from the training process are stored\n","* `model_h5_path` - Path where the model (h5 format) should be stored or loaded from\n","\n","<h3>Speed:</h3>\n","\n","Some processes may take a while depending on the settings and hardware requirements. To speed up the process, certain changes can be made. Obviously, the total amount of data also determines the overall speed. If possible try to use a machine with a GPU - like Google Colab!\n","* [1.2.1 Global variables](#1-2-1) - The easiest way to speed up all processes is to switch to `test_mode`. This will have a strong impact on the results. Lemmatization and pos tagging is **not** disabled in `test_mode`.\n","* [1.2.1 Global variables](#1-2-1) Adjust the parameters to fit your needs\n","    * `epochs` - Iterations for training\n","    * `search_epochs` - Iterations for finding the best hyperparameters\n","    * `executions_per_trial` - Number of models that should be built and fit for each trial for robustness purposes.\n","    * `hyperband_iterations` - The number of times to iterate over the full Hyperband algorithm.\n","* [4.2.1 POS tagging](#4-2-1) - this process uses Scad not be executedpy to tag every word in a sentence. Set `lemmatization_enabled` to ``False` to skip it. \n","* [6.1.1. Prepare values for visualization](#6-1-1) - if `lemmatization_enabled` is set to `True` the list of unique vocabulary for each author is lemmatized. This will slow down the process.\n","\n","<h3>Additional information:</h3>    \n","<br><br>\n","<h2>Content</h2>\n","\n","* [1. Preparations](#1)\n","* [2. Loading text data](#2)\n","* [3. Collect data and create word collection](#3)\n","* [4. Create and extend DataFrame](#4)\n","* [5. Store or load DataFrame](#4)\n","* [6. Visualization of data](#5)\n","* [7. Prepare and split](#6)\n","* [8. Hyperparameter tuning](#7)\n","* [9. Model preparation](#8)\n","* [10. Save or load model](#8)\n","* [11. Evaluation](#11)\n","* [11. TensorBoard](#12)\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kmrfYSe3pcBQ"},"source":["# 1. Preparations"]},{"cell_type":"markdown","metadata":{"id":"KMGd9dgYtBRt"},"source":["Install Keras tuner and Spacy core. You may install more dependencies if you don't run this in Google Colab."]},{"cell_type":"code","metadata":{"id":"DmSWDUvClRyu"},"source":["!rm -rf ./logs/\n","import spacy.cli\n","\n","!pip install nltk\n","!pip install -q -U keras-tuner\n","!pip install tensorboard\n","\n","spacy.cli.download(\"de_core_news_md\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZV2MIIets6R5"},"source":["Only needed for Google Drive and Colab"]},{"cell_type":"code","metadata":{"id":"jRrVEqH_HfJu"},"source":["if 'google.colab' in str(get_ipython()):\n","  from google.colab import drive\n","  drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SJks0l-pmK6g"},"source":["## 1.1. Imports"]},{"cell_type":"code","metadata":{"id":"g9eYykTQedMP"},"source":["import urllib, IPython, os, datetime, re, nltk, tensorboard, operator, random\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout, BatchNormalization \n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.python.keras.callbacks import TensorBoard\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import tensorflow.keras.activations as activations\n","import tensorflow.keras.losses as losses\n","import tensorflow.keras.optimizers as optimizers\n","import kerastuner as kt\n","from keras.utils.vis_utils import plot_model\n","import matplotlib.pyplot as plt\n","from gensim.models import Word2Vec\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from wordcloud import WordCloud\n","from collections import Counter\n","import spacy\n","from spacy.lemmatizer import Lemmatizer\n","from spacy import displacy\n","from spacy.lang.de.stop_words import STOP_WORDS\n","import de_core_news_md"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L3mTvrTLECOP"},"source":["## 1.2. Downloads for NLTK and Spacy"]},{"cell_type":"code","metadata":{"id":"6mjK0f0vEAqU"},"source":["nltk.download('punkt')\n","spacy.prefer_gpu()\n","nlp = de_core_news_md.load()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WcsMYv9UmvFx"},"source":["## 1.2. Magic functions and global variables"]},{"cell_type":"markdown","metadata":{"id":"dc5mMWU2m3Yd"},"source":["Magic functions"]},{"cell_type":"code","metadata":{"id":"AIVvK2YemWMO"},"source":["%matplotlib inline\n","%load_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N1mB7GhEKyiB"},"source":["### **1.2.1. Global variables and paths** <a class=\"anchor\" id=\"1-2-1\"></a>"]},{"cell_type":"markdown","metadata":{"id":"maH5FORsm6L5"},"source":["Set `session_id` for providing unique file names"]},{"cell_type":"code","metadata":{"id":"dN4kYwXzwGu-"},"source":["session_id = datetime.datetime.now().strftime(\"%d/%m/%Y - %H:%M\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qNVTptxz4O-D"},"source":["**This is the place where some information is needed. Please go though the steps and modify the information according to your needs.**"]},{"cell_type":"markdown","metadata":{"id":"Kb3AYQOrKUG1"},"source":["To free some space after the traing and validation data is created set `auto_free_memory` to `True`."]},{"cell_type":"code","metadata":{"id":"wBZFjDwXK0gZ"},"source":["auto_free_memory = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RhgXsR8F0UQk"},"source":["For tesing the notebook set `test_mode` to `True`."]},{"cell_type":"code","metadata":{"id":"n6oFpBKE0dce"},"source":["test_mode = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VBTLEKATvTRA"},"source":["Paths used for storing and loading. Should **never** end with `/` or file ending."]},{"cell_type":"code","metadata":{"id":"tT45G48BK-Ce"},"source":["source_path = '/content/drive/My Drive/RUAK/input/processed'\n","dataframe_path = '/content/drive/My Drive/RUAK/output/dataframe'\n","word2vec_path = '/content/drive/My Drive/RUAK/output/embedding/w2v'\n","w2v_model_name = 'full_700_iter100_win7_8'\n","hyperband_tuner_output_path = '/content/drive/My Drive/RUAK/output/hp_tuning'\n","checkpoint_path = '/content/drive/My Drive/RUAK/output/training_checkpoints'\n","model_h5_path = '/content/drive/My Drive/RUAK/output/models'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WdAePzg_nM9i"},"source":["List of files to process and author names. Files should be named after author (e.g. `plato.txt`). `file_names` should contain at least 3 files."]},{"cell_type":"code","metadata":{"id":"NW35U-2oXscx"},"source":["file_names = [\n","    'kant.txt', \n","    'nietzsch.txt', \n","    'platon.txt', \n","    'rousseau.txt']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ofC-NjwR4JAD"},"source":["Parameters needed for tuning and training."]},{"cell_type":"code","metadata":{"id":"9sHv768B348B"},"source":["batch_size=40\n","epochs=30\n","search_epochs=20\n","early_stopping_patience=5\n","executions_per_trial=3\n","hyperband_iterations=3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fBdj3KzTqJXz"},"source":["## 1.3 [Stop words](https://en.wikipedia.org/wiki/Stop_word)"]},{"cell_type":"code","metadata":{"id":"kOifz08hn-KO"},"source":["def replace_umlaut(string):\n","    string = string.replace('ä', 'ae')\n","    string = string.replace('ö', 'oe')\n","    string = string.replace('ü', 'ue')\n","    string = string.replace('Ä', 'Ae')\n","    string = string.replace('Ö', 'Oe')\n","    string = string.replace('Ü', 'Ue')\n","    return string.replace('ß', 'ss')\n","\n","stop_words = set([replace_umlaut(word) for word in STOP_WORDS])\n","print(f'Stop words count: {len(stop_words)}.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y9yIECZO91cG"},"source":["# 2. Loading text data"]},{"cell_type":"code","metadata":{"id":"tgTeoYVTRrw9"},"source":["if len(file_names) < 3:\n","  raise ValueError(\"'file_names' should contain at least 3 files. Add more files at (1.2.1)!\")\n","\n","if test_mode == True:\n","  file_names = file_names[0:3]\n","\n","for file_name in file_names:\n","  text_dir = tf.keras.utils.get_file(file_name, origin=f'file://{source_path}/{file_name}')\n","\n","parent_dir = os.path.dirname(text_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"96g2waSU9t-i"},"source":["# 3. Collect data and create word collections"]},{"cell_type":"markdown","metadata":{"id":"UYahXMunoDtD"},"source":["## 3.1. Prepare word collections"]},{"cell_type":"code","metadata":{"id":"ddeJd1CW1Bxw"},"source":["author_names = [name[:-4].capitalize() for name in file_names]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KeNOOlWpTu71"},"source":["Function to add words to `words_without_stop_words` and `unique_words_without_stop_words`."]},{"cell_type":"code","metadata":{"id":"R7vT1iazMBIU"},"source":["def add_words(sentence):\n","  for word in sentence.split():\n","    word = re.sub(r\"[^a-zA-Z]+\", \"\", word)\n","    if word == '' or len(word) == 1:\n","      continue\n","    if word.lower() not in stop_words:\n","      words_without_stop_words.append(word)\n","      unique_words_without_stop_words.add(word)\n","\n","words_without_stop_words = []\n","unique_words_without_stop_words = set()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8xaLeKDYq5hX"},"source":["## 3.2. Extract sentences"]},{"cell_type":"markdown","metadata":{"id":"wGNi3Yh7mt3y"},"source":["Extract sentences from files and creates labels list. Adjust the language for the `nltk.sent_tokenizer` if needed."]},{"cell_type":"code","metadata":{"id":"gdnFXAtFdYrG"},"source":["labels = []\n","sentences = []\n","\n","for index, file_name in enumerate(file_names):\n","\n","  path = os.path.join(parent_dir, file_name)\n","\n","  with open(path, 'rb') as file: \n","    text = str(file.read())\n","    nltk_sentences = nltk.sent_tokenize(text, language='german')\n","\n","    for sentence in nltk_sentences:\n","      sentence = str(sentence).replace(\"b'\", \"\")\n","      sentences.append(sentence)\n","      labels.append(index)\n","      add_words(sentence)\n","\n","    print(f\"Sentences for {file_name} with label: {index} added.\")\n","\n","print(f'\\n{len(sentences)} sentences found.')\n","print(f'{len(words_without_stop_words)} words found (excl. stop words).')\n","print(f'{len(unique_words_without_stop_words)} unique words found (excl. stop words).')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"awuDoZibMw6p"},"source":["Collect most commen words except the stop words."]},{"cell_type":"code","metadata":{"id":"9CINAlwkMf4M"},"source":["most_common = [word[0] for word in Counter(words_without_stop_words).most_common(20)]\n","most_common_count = {k: v for k, v in Counter(words_without_stop_words).most_common(20)}\n","print(most_common)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oVfu1MwGocix"},"source":["## 3.3. Clean data"]},{"cell_type":"code","metadata":{"id":"IDwZJ9EDohfs"},"source":["def short_sentences(length):\n","  short_sentences = [sentence for sentence in sentences if len(sentence.split()) <= length]\n","  print(f'Found {len(short_sentences)} sentences shorter than {length} words.\\n')\n","  return short_sentences\n","\n","def long_sentences(length):\n","  long_sentences = [sentence for sentence in sentences if len(sentence.split()) >= length]\n","  print(f'Found {len(long_sentences)} sentences longer than {length} words.\\n')\n","  return long_sentences"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bhtTOWyarJDd"},"source":["### 3.3.1. Remove sentences"]},{"cell_type":"markdown","metadata":{"id":"3n3i-snVro6O"},"source":["Set min and max length for sentences"]},{"cell_type":"code","metadata":{"id":"VRuSh6_SrSSj"},"source":["min_length = 6\n","max_length = 400"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JYAlG39IsLHv"},"source":["Get invalid sentences"]},{"cell_type":"code","metadata":{"id":"A9z6pG3Fryce"},"source":["invalid_sentences = short_sentences(min_length) + long_sentences(max_length)\n","print(f'Found {len(invalid_sentences)} invalid sentences.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lhm1YKpXtGe5"},"source":["#### 3.3.1.1. Investigate invalid sentences\n","Print 5 examples of `invalid_sentences`"]},{"cell_type":"code","metadata":{"id":"bhPflHoDtLyp"},"source":["for i in random.sample(range(10, len(invalid_sentences)-1), 10):\n","  print(invalid_sentences[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HpemuuR72jxm"},"source":["Use Spacy [Visualizer](https://spacy.io/usage/visualizers) to show a random invalid sentence."]},{"cell_type":"code","metadata":{"id":"4oDAA_zi1cFm"},"source":["doc = nlp(invalid_sentences[random.randint(0, len(invalid_sentences)-1)])\n","displacy.render(doc, style=\"dep\", jupyter=True, options={'compact':'True'})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nTg_hLTivcyj"},"source":["### 3.3.2. Provide cleaned data"]},{"cell_type":"code","metadata":{"id":"GlUS_PuEvnvM"},"source":["cleaned_labels = []\n","cleaned_sentences = []\n","print(f\"'sentences' list length before removal: {len(sentences)}.\")\n","for index, sentence in enumerate(sentences):\n","  if sentence not in invalid_sentences:\n","    cleaned_sentences.append(sentence)\n","    cleaned_labels.append(labels[index])  \n","print(f\"'sentences' list length after removal: {len(cleaned_sentences)}.\")\n","print(f'{len(invalid_sentences)} sentences removed.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"muvXsp-L2RjH"},"source":["# 4. Create and extend DataFrame"]},{"cell_type":"markdown","metadata":{"id":"fxUWXnD72gHL"},"source":["Some helper methods"]},{"cell_type":"code","metadata":{"id":"10sfumjtrcsw"},"source":["def stop_word_quote_fn(sentence):\n","  count = 0\n","  for word in sentence.split():\n","    word = re.sub(r\"[^a-zA-Z]+\", \"\", word)\n","    if word.lower() in stop_words:\n","      count += 1\n","  return round(count/len(sentence.split()) * 100, 2)\n","\n","def stop_word_count_fn(sentence):\n","  count = 0\n","  for word in sentence.split():\n","    word = re.sub(r\"[^a-zA-Z]+\", \"\", word)\n","    if word.lower() in stop_words:\n","      count += 1\n","  return count\n","\n","def mean_word_length_fn(sentence):\n","  return round(np.array([len(word) for word in sentence.replace('.','').split()]).mean(), 2)\n","\n","def pos_count(sentence, pos):\n","  doc = nlp(sentence)\n","  return len([w.pos_ for w in doc if w.pos_ == pos])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZxICrBEKbZE1"},"source":["## 4.1. Create DataFrame"]},{"cell_type":"code","metadata":{"id":"uFs6VfqrbQzg"},"source":["df = pd.DataFrame({'label': cleaned_labels, 'sentence': cleaned_sentences})\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8rsP0kemG7wE"},"source":["Remove 90% of the rows for `test_mode`"]},{"cell_type":"code","metadata":{"id":"K6jm_w6yHBFM"},"source":["if test_mode == True:\n","  print(f'Before drop: {df.shape}')\n","  df = df.drop(df.sample(frac=0.9).index)\n","  print(f'After drop: {df.shape}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XtHcxdysQuPe"},"source":["## 4.2. Construct new data"]},{"cell_type":"code","metadata":{"id":"qql42ncCQtK5"},"source":["df['author'] = df['label'].map(lambda x: author_names[x])\n","df['word_count'] = df['sentence'].str.split().str.len()\n","df['mean_word_length'] = df['sentence'].map(mean_word_length_fn)\n","df['stop_words_ratio'] = df['sentence'].map(stop_word_quote_fn)\n","df['stop_words_count'] = df['sentence'].map(stop_word_count_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j-PMNxYGUHj_"},"source":["### 4.2.1 POS tagging <a class=\"anchor\" id=\"4-2-*1*\"></a>\n","Add columns and values for [POS tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging). Annotations can be found [here](https://spacy.io/api/annotation). **This may take a while!**"]},{"cell_type":"code","metadata":{"id":"KwCwoFniz8l8"},"source":["pos_tagging_enabled = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oaAHSANZUG9J"},"source":["if pos_tagging_enabled == True:\n","  pos_tags = ['ADJ', 'ADV', 'ADP', 'AUX', 'DET', 'NUM', 'X', 'INTJ', 'CCONJ',\n","              'SCONJ', 'PROPN', 'NOUN', 'DET', 'PRON', 'PART', 'VERB']\n","  for tag in pos_tags:\n","    df[f'{tag}_count'] = df['sentence'].map(lambda sen: pos_count(sen, tag))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4JYzLWqkrh_Y"},"source":["## 4.3. Preview processed DataFrame"]},{"cell_type":"code","metadata":{"id":"7mXU1V4GazIh"},"source":["df.head(df.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1t6t2UGRo1Jg"},"source":["# 5. Store or load DataFrame"]},{"cell_type":"markdown","metadata":{"id":"PYrSJXlMn5Q_"},"source":["Save DataFrame to CSV if needed."]},{"cell_type":"code","metadata":{"id":"yUD-BM87n9tJ"},"source":["if test_mode == False:\n","  df.to_csv(f'{dataframe_path}/ruak_dataframe.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FtFcnhfko9wQ"},"source":["Load the DataFrame from CSV if needed."]},{"cell_type":"code","metadata":{"id":"naVqKAOXpDoL"},"source":["if test_mode == False:\n","  df = pd.read_csv(f'{dataframe_path}/ruak_dataframe.csv')\n","  df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Fo0XiX7S6dr"},"source":["# 6. Visualization of data"]},{"cell_type":"markdown","metadata":{"id":"DkQFhI6J0Sw_"},"source":["## 6.1 Prepare values for visualization"]},{"cell_type":"markdown","metadata":{"id":"Dg-2j4xO_Ua7"},"source":["Count vocabulary"]},{"cell_type":"code","metadata":{"id":"GpYhWH3Njg0h"},"source":["def vocabulary_count_fn(series, lemmatization):\n","  vocabulary = set()\n","  for sentence in series:\n","    if lemmatization == True:\n","      words = lemmatize(sentence)\n","    else:\n","      words = sentence.split()  \n","    for word in words:\n","      if word.lower() not in stop_words:\n","        word = re.sub(r'[^a-zA-Z]+', '', word)\n","        vocabulary.add(word.lower())\n","  return len(vocabulary)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"63Nrf9EI1ugk"},"source":["[Lemmatize](https://en.wikipedia.org/wiki/Lemmatisation)"]},{"cell_type":"code","metadata":{"id":"COZcha201wQP"},"source":["def lemmatize(sentence):\n","  words = set()\n","  doc = nlp(sentence)\n","  for word in doc:\n","    words.add(word.lemma_)\n","  return list(words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4JYXioUBSsAu"},"source":["### 6.1.1. Prepare values for visualization <a class=\"anchor\" id=\"6-1-1\"></a>"]},{"cell_type":"code","metadata":{"id":"PdHeUoJdi5Gp"},"source":["lemmatization_enabled = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CH14xXHg_i0c"},"source":["Prepare values for visualization. Enable lemmatization to get more a more prezise `unique_vocabulary_count`. **This will slow down the process!**"]},{"cell_type":"code","metadata":{"id":"XieassmAz9fb"},"source":["median_sentence_length = df.groupby('label')['word_count'].median()\n","mean_stop_words = df.groupby('label')['stop_words_ratio'].mean()\n","sentence_count = df.groupby('label')['sentence'].count()\n","unique_vocabulary_count = df.groupby('label')['sentence'].apply(lambda ser: vocabulary_count_fn(ser, lemmatization_enabled))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z3e6981g09wl"},"source":["## 6.2. Draw visualization"]},{"cell_type":"markdown","metadata":{"id":"wNVDqenRPHpg"},"source":["### 6.2.1 Data distribution\n","The data should be equally split between authors."]},{"cell_type":"code","metadata":{"id":"Ch_70OxpOoYG"},"source":["plt.pie(df['author'].value_counts(),\n","        explode=np.full(len(author_names), 0.1),\n","        radius=2,\n","        autopct='%1.0f%%', \n","        labels=author_names,\n","        shadow=True,\n","        startangle=90,\n","        textprops={'size': 15})\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6TRUVgAr_7j9"},"source":["### 6.2.2. Comparing authors"]},{"cell_type":"code","metadata":{"id":"oJkpwQ6IUyKD"},"source":["fig, axs = plt.subplots(4,1, figsize=(10,15))\n","fig.tight_layout(h_pad=6)\n","\n","axs[0].bar(author_names, sentence_count)\n","axs[0].set_ylabel('Number of sentences', fontdict={'color':'gray', 'size':12})\n","axs[0].tick_params(axis='both', colors='gray', labelsize=12)\n","axs[0].grid()\n","\n","axs[1].bar(author_names, median_sentence_length)\n","axs[1].set_ylabel('Median sentence lenth', fontdict={'color':'gray', 'size':12})\n","axs[1].tick_params(axis='both', colors='gray', labelsize=12)\n","axs[1].grid()\n","\n","axs[2].bar(author_names, unique_vocabulary_count)\n","axs[2].set_ylabel('Unique vocabulary count\\n(excl. stop words)', fontdict={'color':'gray', 'size':12})\n","axs[2].tick_params(axis='both', colors='gray', labelsize=12)\n","axs[2].grid()\n","\n","axs[3].bar(author_names, mean_stop_words)\n","axs[3].set_ylabel('Median stop words ratio', fontdict={'color':'gray', 'size':12})\n","axs[3].tick_params(axis='both', colors='gray', labelsize=12)\n","axs[3].grid()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QdmHHaPJ_zdx"},"source":["### 6.2.3. Common words in sentences"]},{"cell_type":"code","metadata":{"id":"yUqVaJBpR8mW"},"source":["fig, axs = plt.subplots(1,1, figsize=(30,10))\n","\n","axs.bar(most_common_count.keys(), most_common_count.values(), color='orange')\n","axs.set_ylabel('Number of sentences', fontdict={'color':'gray', 'size':17})\n","axs.tick_params(axis='both', colors='gray', labelsize=17)\n","axs.grid()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NhWNIAP2azJH"},"source":["### 6.2.4. Visualize sentences for authors\n","This shows the sentence structure, lemmas, and pos tags of one random sentences from each author."]},{"cell_type":"code","metadata":{"id":"cAo74rFzaz6L"},"source":["for author in author_names:\n","  sentences_series = df.loc[(df['author'] == 'Kant') & (df['sentence'].str.len() < 50)]['sentence']\n","  print(f'\\nSentence by {author}:')\n","  doc = nlp(sentences_series.sample(n=1).values[0])\n","  displacy.render(doc, \n","                  style=\"dep\", \n","                  jupyter=True, \n","                  options={'compact':'True', 'add_lemma': 'True'})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8YEfw66S_oVo"},"source":["### 6.2.5. Word cloud"]},{"cell_type":"code","metadata":{"id":"QmPbGhTn5tHy"},"source":["wordcloud = WordCloud(width=5000, \n","                      height=4000,\n","                      max_words=20,  \n","                      background_color ='black', \n","                      stopwords = stop_words, \n","                      min_font_size = 10).generate_from_frequencies(most_common_count) \n","\n","plt.figure(figsize=(20, 12), facecolor='k', edgecolor ='k') \n","plt.imshow(wordcloud) \n","plt.axis(\"off\") \n","plt.tight_layout(pad=0) \n","plt.show() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"61D3POnhr3fT"},"source":["# 7. Prepare and split"]},{"cell_type":"markdown","metadata":{"id":"Aad2fR1Rs5bn"},"source":["## 7.1. Tokenize"]},{"cell_type":"code","metadata":{"id":"TKA1Om5OtNFd"},"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df['sentence'].values)\n","print(f\"{len(df['sentence'].values)} sentences from {len(file_names)} authors.\")\n","print(f'{len(tokenizer.word_counts)} unique vocabularies.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JwIYTz819jIk"},"source":["## 7.2. Encode"]},{"cell_type":"code","metadata":{"id":"DaLmBJfP73gJ"},"source":["encoded_sentences = tokenizer.texts_to_sequences(df['sentence'].values)\n","padded_sentences = pad_sequences(encoded_sentences, padding='post')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s5mvazkEy4pF"},"source":["Test the encoder"]},{"cell_type":"code","metadata":{"id":"ZeS-BVU9y6d7"},"source":["print(df['sentence'].values[0])\n","print(np.array(padded_sentences[0]))\n","print(tokenizer.sequences_to_texts([padded_sentences[0]]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dZMGHboj8yRE"},"source":["## 7.3. Splitting\n","Create train and test data for the fitting proccess."]},{"cell_type":"code","metadata":{"id":"5uHhk9JCoyXD"},"source":["X_train, X_valid, y_train, y_valid = train_test_split(padded_sentences, df['label'].values, test_size=0.1)\n","print(f'Shape of the splited X_train: {X_train.shape}')\n","print(f'Shape of the splited y_train: {y_train.shape}')\n","print(f'Shape of the splited X_valid: {X_valid.shape}')\n","print(f'Shape of the splited y_valid: {y_valid.shape}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PVVYN4t5kJU2"},"source":["# 8. Hyperparameter tuning"]},{"cell_type":"markdown","metadata":{"id":"WR3UNPa4Vetw"},"source":["Free some space"]},{"cell_type":"code","metadata":{"id":"CnQJvyIsVfGQ"},"source":["if auto_free_memory == True:\n","  del df\n","  del wordcloud\n","  del encoded_sentences\n","  del sentences\n","  del labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xur6_r2MJ2QE"},"source":["## 8.1. Setup the hypermodel\n","Load the Word2Vec model for providing the weights for the embedding layer."]},{"cell_type":"code","metadata":{"id":"7Y9YgOClCcyD"},"source":["def embedding_matrix():\n","    model = Word2Vec.load(f'{word2vec_path}/{w2v_model_name}.model')\n","    embedding_matrix = np.zeros((len(model.wv.vocab), model.vector_size))\n","    \n","    for i in range(len(model.wv.vocab)):\n","        embedding_vector = model.wv[model.wv.index2word[i]]\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","            \n","    print(f'Embedding_matrix shape: {embedding_matrix.shape}')\n","    del model\n","    return embedding_matrix\n","\n","embedding_matrix = embedding_matrix()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZU_5ktAbtXUz"},"source":["Define the model"]},{"cell_type":"code","metadata":{"id":"i5uA91DTI6I0"},"source":["def hypermodel(hp):\n","\n","  if test_mode == True:\n","    hp_dense_count = hp.Int('dense_count', min_value=1, max_value=2, step=1)\n","    hp_embedding_trainable = hp.Choice('embedding_trainable', [False])\n","    hp_with_batch_normalization = hp.Choice('with_batch_normalization', [True])\n","    hp_lstm_units = hp.Int('lstm_units', 32, 64, step=32)\n","    hp_dropout = hp.Choice('dropout', [0.25])\n","    hp_learning_rate = hp.Choice('learning_rate', [0.001])\n","    hp_adam_epsilon = hp.Choice('adam_epsilon', values=[1e-08])\n","  else:\n","    hp_dense_count = hp.Int('dense_count', min_value=1, max_value=7, step=1)\n","    hp_embedding_trainable = hp.Choice('embedding_trainable', [True, False])\n","    hp_with_batch_normalization = hp.Choice('with_batch_normalization', [True, False])\n","    hp_lstm_units = hp.Int('lstm_units', 256, 512, step=128)\n","    hp_dropout = hp.Choice('dropout', [0.0, 0.1, 0.25, 0.5])\n","    hp_learning_rate = hp.Choice('learning_rate', [0.01, 0.001, 0.0001])\n","    hp_adam_epsilon = hp.Choice('adam_epsilon', values=[1e-07, 1e-08])\n","\n","  model = tf.keras.Sequential()\n","                                    \n","  model.add(Embedding(len(embedding_matrix),\n","                    output_dim=700,\n","                    weights=[embedding_matrix], \n","                    trainable=hp_embedding_trainable,\n","                    mask_zero=True))\n","  \n","  model.add(Bidirectional(LSTM(hp_lstm_units, return_sequences=True)))\n","  if hp_embedding_trainable == True:\n","    BatchNormalization()\n","  model.add(Dropout(hp_dropout))\n","\n","  model.add(Bidirectional(LSTM(hp_lstm_units, return_sequences=True)))\n","  if hp_embedding_trainable == True:\n","    BatchNormalization()  \n","  model.add(Dropout(hp_dropout))\n","\n","  model.add(Bidirectional(LSTM(hp_lstm_units, return_sequences=False)))\n","  if hp_embedding_trainable == True:\n","    BatchNormalization()  \n","  model.add(Dropout(hp_dropout))\n","\n","  for i in range(hp_dense_count):\n","\n","    if test_mode == True:\n","      hp_dense_units = hp.Int(f'dense_units{i}', 64, 128, step=64)\n","      hp_dense_activation = hp.Choice(f'dense_activation_{i}', values=['relu'])\n","    else: \n","      hp_dense_units = hp.Int(f'dense_units{i}', 64, 512, step=64)\n","      hp_dense_activation = hp.Choice(f'dense_activation_{i}', values=['tanh', 'relu'])\n","\n","    model.add(Dense(hp_dense_units, activation=hp_dense_activation))\n","\n","  model.add(Dense(len(file_names), activation='softmax'))\n","\n","  model.compile(optimizer=optimizers.Adam(learning_rate=hp_learning_rate, epsilon=hp_adam_epsilon),\n","              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2v8MXrJrwRmp"},"source":["## 8.2. Run the tuner\n","Reduce parameters for `testing_mode`"]},{"cell_type":"code","metadata":{"id":"uJOuo19A16Mh"},"source":["if test_mode == True:\n","  epochs=4\n","  search_epochs=1\n","  early_stopping_patience=4\n","  executions_per_trial=1\n","  hyperband_iterations=1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GbHsa2qdUKxb"},"source":["Set variables"]},{"cell_type":"code","metadata":{"id":"eeoGHkEgUNjs"},"source":["max_epochs = epochs+5\n","project_name = 'RUAK'\n","verbose = 2\n","if test_mode == True:\n","  max_epochs = 1\n","  project_name = 'RUAK_testing'\n","  verbose = 0\n","  print('Running in test mode!')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W9Gd2vHVRvNw"},"source":["Prepare the hyperband tuner"]},{"cell_type":"code","metadata":{"id":"PvrDPbJsMN7A"},"source":["tuner = kt.Hyperband(hypermodel,\n","                     objective='val_accuracy', \n","                     executions_per_trial=executions_per_trial,\n","                     max_epochs=max_epochs,\n","                     hyperband_iterations=hyperband_iterations,\n","                     directory=hyperband_tuner_output_path,\n","                     project_name=project_name,\n","                     overwrite=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lr7vVHUiT4lE"},"source":["Run the tuner to search for best parameters. The result are the optimal hyperparameters: `best_hps` and a list of `best_models`."]},{"cell_type":"code","metadata":{"id":"9raFi0ERTp5E"},"source":["class ClearTrainingOutput(tf.keras.callbacks.Callback):\n","  def on_train_end(*args, **kwargs):\n","    IPython.display.clear_output(wait=True)\n","\n","tuner.search(X_train, y_train, \n","             epochs=search_epochs,\n","             validation_data = (X_valid, y_valid),\n","             callbacks = [ClearTrainingOutput(), EarlyStopping('val_accuracy', patience=1)],\n","             verbose=verbose)\n","\n","best_hps = tuner.get_best_hyperparameters(1)[0]\n","best_models = tuner.get_best_models(num_models=3)\n","\n","tuner.results_summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u2PdigVLCdcL"},"source":["# 9. Model preparation"]},{"cell_type":"markdown","metadata":{"id":"T0RQAk0PbU_X"},"source":["Get summaries of the best models and choose the model for training."]},{"cell_type":"code","metadata":{"id":"PwVKYs2XzpUp"},"source":["for model in best_models:\n","  model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gyx6tUlnzK1M"},"source":["Choose preferred model"]},{"cell_type":"code","metadata":{"id":"zUMY2E68bUZ-"},"source":["chosen_model = best_models[0]\n","del best_models"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-_Tur-i_y5fB"},"source":["Plot model structure"]},{"cell_type":"code","metadata":{"id":"_c0wvzcRy4xO"},"source":["plot_model(chosen_model, show_shapes=True, show_layer_names=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hz9f9MAqhg6c"},"source":["## 9.1. Prepare callbacks\n","TensorBoard preparation"]},{"cell_type":"code","metadata":{"id":"oX7suDrVRiYq"},"source":["log_dir = os.path.join('logs', session_id)\n","tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_E6Aba9xuinl"},"source":["Create `ModelCheckpoint` and `EarlyStopping` callbacks."]},{"cell_type":"code","metadata":{"id":"mCZgMlv-hkaU"},"source":["cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f'{checkpoint_path}/cp.ckpt',\n","                                                 save_weights_only=True,\n","                                                 verbose=1)\n","\n","es_callback = EarlyStopping('val_accuracy', patience=early_stopping_patience, restore_best_weights=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S4U6I-8lCivy"},"source":["## 9.2. Model training"]},{"cell_type":"code","metadata":{"id":"6gksI7UJCce6"},"source":["callbacks = [cp_callback, es_callback, tb_callback]\n","if test_mode == False:\n","    callbacks = [es_callback, tb_callback]\n","    print('Running in test mode!')\n"," \n","h = chosen_model.fit(X_train, \n","                      y_train, \n","                      epochs=epochs, \n","                      batch_size=batch_size, \n","                      validation_data=(X_valid, y_valid), \n","                      callbacks=callbacks,\n","                      verbose=verbose)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KpOp6znCCCpu"},"source":["# 10. Save or load model"]},{"cell_type":"markdown","metadata":{"id":"AYJ4k-VLKHdo"},"source":["## 10.1. Save model"]},{"cell_type":"code","metadata":{"id":"y4TgjSFvKG7c"},"source":["if test_mode == False:\n","  model.save(f'{model_h5_path}/ruak_model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dua2WMwGCHFd"},"source":["## 10.2. Load model"]},{"cell_type":"code","metadata":{"id":"iAh4a9qiCFew"},"source":["if test_mode == False:\n","  chosen_model = tf.keras.models.load_model(f'{model_h5_path}/ruak_model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4jQajX37z3bO"},"source":["## 10.3. Load weights"]},{"cell_type":"code","metadata":{"id":"vwyOI709_Tu6"},"source":["if test_mode == False:\n","  latest = tf.train.latest_checkpoint(f'{checkpoint_path}/cp.ckpt')\n","  chosen_model.load_weights(latest)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uHHVXU4C7nc9"},"source":["# 11. Evaluation"]},{"cell_type":"markdown","metadata":{"id":"WA-xq__FeVJg"},"source":["Draw charts to show compare training and validation data"]},{"cell_type":"code","metadata":{"id":"k0_FQ4v3X3iz"},"source":["fig, axs = plt.subplots(2,1, figsize=(8, 6))\n","\n","epochs = range(len(h.history['accuracy']))\n","axs[0].plot(epochs, h.history['accuracy'], color='red', marker='x')\n","axs[0].plot(epochs, h.history['val_accuracy'], color='green', marker='.')\n","axs[0].legend(labels=['Training accuracy','Validation accuracy'])\n","axs[0].set_ylabel('Accuracy', fontdict={'color':'gray', 'size':12})\n","axs[0].tick_params(labelbottom=False)\n","axs[0].grid()\n","\n","epochs = range(len(h.history['loss']))\n","axs[1].plot(epochs, h.history['loss'], color='red', marker='x')\n","axs[1].plot(epochs, h.history['val_loss'], color='green', marker='.')\n","axs[1].legend(labels=['Training loss','Validation loss'])\n","axs[1].set_ylabel('Loss', fontdict={'color':'gray', 'size':12})\n","axs[1].tick_params(labelbottom=False)\n","axs[1].grid()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9UwQPCjCvscA"},"source":["Show loss and accuracy"]},{"cell_type":"code","metadata":{"id":"1yB-R-XK7rEx"},"source":["val_loss, val_acc = chosen_model.evaluate(X_valid)\n","\n","print(f'Validation Accuracy: {val_acc}')\n","print(f'Validation Loss: {val_loss}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RiHTWqlnv3Wi"},"source":["## 11.1. Test the model"]},{"cell_type":"markdown","metadata":{"id":"UAv-WNEmMxti"},"source":["Test the model. Add  `sample_sentences` to get the probability distribution for each author."]},{"cell_type":"code","metadata":{"id":"BpLozmZOGU7I"},"source":["sample_sentences = [\n","                    # 0 - Platon\n","                    \"Die steht es also, da sich jetzt die Lebensweise unserer Helfer ja weit schoener und vortrefflicher zeigt als die der olympischen Sieger, kann man sie wohl auch nur vergleichen mit dem Leben der Schuster oder der uebrigen Handwerker oder der Landwirte?\",\n","                    # 1 - Nietzsche\n","                    \"Ich lege besonderen Accent darauf hervorzuheben, dass mein Bruder den Antichrist veroeffentlicht hat und dass er wahrscheinlich urspruenglich in einer milderen Tonart niedergeschrieben wurde.\", \n","                    # 2 - Kant\n","                    \"Ich habe einen Vorwurf gewaehlt, welcher sowohl von Seiten seiner innern Schwierigkeit, als auch in Ansehung der Religion einen grossen Theil der Leser gleich anfaenglich mit einem nachtheiligen Vorurtheile einzunehmen vermoegend ist.\"\n","                   ]\n","\n","# Add your own sentences:\n","# sample_sentences = [\"Here you can try your own sentences.\", \"Let's see what kind of philosopher you are.\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VhElQ-nKN7em"},"source":["encoded_sample_sentences = tokenizer.texts_to_sequences(sample_sentences)\n","padded_sample_sentences = pad_sequences(encoded_sample_sentences, maxlen=X_train.shape[1], padding='post')\n","predictions = model.predict(padded_sample_sentences)\n","\n","predictions_df = pd.DataFrame()\n","for index, prediction in enumerate(predictions):\n","  for i, pre in enumerate(prediction):\n","    predictions_df = predictions_df.append({\n","      'sentence_number': index,\n","      'author': author_names[i],\n","      'prediction': pre,\n","      'sentence': sample_sentences[index]\n","    }, ignore_index=True)\n","\n","predictions_df.head(predictions_df.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EwMK3z6r4CQh"},"source":["Draw bars for each sample sentence"]},{"cell_type":"code","metadata":{"id":"Tq8-cebk4BaH"},"source":["fig, axs = plt.subplots(len(predictions), 1, figsize=(10,10))\n","fig.tight_layout(h_pad=6)\n","for sen_id, pre in enumerate(predictions):\n","  for i, p in enumerate(pre):\n","    axs[sen_id].barh(author_names, pre)\n","    axs[sen_id].set_title(f'Sentence {sen_id}: {sample_sentences[sen_id][0:70]}...', \n","                          fontdict={'color':'gray', 'size':12, 'fontweight':'bold'})\n","    axs[sen_id].set_ylabel('Author', fontdict={'color':'gray', 'size':12})\n","    axs[sen_id].set_xlabel('Probability', fontdict={'color':'gray', 'size':12})\n","    \n","    axs[sen_id].tick_params(axis='both', colors='gray', labelsize=12)\n","    axs[sen_id].grid()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l5TqibMKjSmM"},"source":["# 12. TensorBoard <a class=\"anchor\" id=\"12\"></a>"]},{"cell_type":"code","metadata":{"id":"8oJRvGKH7KmT"},"source":["%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JxG2o_axywgE"},"source":["Clean TensorBoad logs"]},{"cell_type":"code","metadata":{"id":"0o8W7ogaywG0"},"source":["!rm -rf ./logs/"],"execution_count":null,"outputs":[]}]}