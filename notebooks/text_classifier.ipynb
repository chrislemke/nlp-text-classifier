{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text_classifier.ipynb","provenance":[{"file_id":"1-GLcGn8_wvQsyMD6VCOHgtZ_Zsw3eqsr","timestamp":1589599796682}],"private_outputs":true,"collapsed_sections":["L3mTvrTLECOP","WcsMYv9UmvFx","fBdj3KzTqJXz","Y9yIECZO91cG","96g2waSU9t-i","muvXsp-L2RjH","0Fo0XiX7S6dr","Z3e6981g09wl","61D3POnhr3fT","Aad2fR1Rs5bn","JwIYTz819jIk"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"64viHxknCk30"},"source":["# 0. Introduction"]},{"cell_type":"markdown","metadata":{"id":"Ygny9kadCq_w"},"source":["<h1>RUAK - Are you a Hegel?</h1>\n","\n","> The greatest challenge to any thinker is stating the problem in a way that will allow a solution.\n","\n","<a href=\"https://en.wikipedia.org/wiki/Bertrand_Russell\">Bertrand Russell</a>\n","<br><br>\n","<h2>About the project</h2>\n","Philosophy is a fundamental human thought movement. Everyone is a philosopher. The only question is what kind of philosopher you are. This project tries to answer that question.\n","Using natural language processing (NLP), texts of different authors are used for categorization.\n","With the help of these texts any sentence can be categorically determined.\n","To understand how written language works and what the differents are between authors it helps to analyse the context of the sentences. Though visualization it is simpler to see structural varieties such as average sentence length, word class ratio and the use of <a href=\"https://en.wikipedia.org/wiki/Stop_word\">stop words</a>.\n","<br><br>\n","<h2>About this notebook</h2>\n","You can open this Jupyter notebook in Google Colab to use a GPU and have a nice platform for editing.\n","<br>\n","<a href=\"https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>\n","\n","<br>\n","\n","<h2>Information on use</h2>\n","<h3>Paths:</h3>\n","The following data needs to be loaded. Please adjust the paths accordingly (1.2.1): \n","<ul>\n","<li><code>source_path</code> - Path which contains the text files</li>\n","<li><code>dataframe_file_path</code> - Path for loading and saving the DataFrame</li>\n","<li><code>word2vec_path</code> - Path to Word2Vec model</li>\n","<li><code>hyperband_tuner_output_path</code> - Path to hyperparameter tuner working directory</li>\n","<li><code>checkpoint_path</code> - Path where the checkpoints from the training process are stored</li>\n","<li><code>model_h5_path</code> - Path where the model (h5 format) should be stored or loaded from</li>\n","</ul>\n","<br>\n","<h3>Speed:</h3>\n","Some processes may take a while depending on the settings and hardware requirements. To speed up the process, certain changes can be made. Obviously, the total amount of data also determines the overall speed. If possible try to use a machine with a GPU - like Google Colab!\n","<ul>\n","<li>The easiest way to speed up all processes is to switch to <code>test_mode</code> (1.2.1). This will have a strong impact on the results. Lemmatization and pos tagging is <b>not</b> disabled in <code>test_mode</code>.\n","<li>Adjust the parameters to fit your needs (1.2.1)\n","  <ul>\n","    <li><code>epochs</code> - Iterations for training</li>\n","    <li><code>search_epochs</code> - Iterations for finding the best hyperparameters</li>\n","    <li><code>executions_per_trial</code> - Number of models that should be built and fit for each trial for robustness purposes.</li>\n","  </ul>   \n","</li>\n","<li><code>hyperband_iterations</code> - The number of times to iterate over the full Hyperband algorithm.</li>\n","<li>POS tagging - this process uses Scad not be executedpy to tag every word in a sentence (4.2.1). Set <code>lemmatization_enabled</code> to <code>False</code> to skip it.</li>\n","<li>Prepare values for visualization (6.1.1.) - if <code>lemmatization_enabled</code> is set to <code>True</code> the list of unique vocabulary for each author is lemmatized. This will slow down the process.</li>\n","</ul>\n","<br>\n","<h3>Additional information:</h3>    \n","<br><br>\n","<h2>Content</h2>\n","\n","* [1. Preparations](#1)\n","* [2. Loading text data](#2)\n","* [3. Collect data and create word collection](#3)\n","* [4. Create and extend DataFrame](#4)\n","* [5. Store or load DataFrame](#4)\n","* [6. Visualization of data](#5)\n","* [7. Prepare and split](#6)\n","* [8. Hyperparameter tuning](#7)\n","* [9. Model preparation and training](#8)\n","* [10. Save or load model](#8)\n","* [11. Evaluation](#11)\n","* [12. TensorBoard](#12)\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kmrfYSe3pcBQ"},"source":["# 1. Preparations\n","**Set the language**: `english` or `german`"]},{"cell_type":"code","metadata":{"id":"GROFjJ0uT2FD"},"source":["language = 'english'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KMGd9dgYtBRt"},"source":["Install Keras tuner and Spacy core. You may install more dependencies if you don't run this in Google Colab."]},{"cell_type":"code","metadata":{"id":"DmSWDUvClRyu"},"source":["!rm -rf ./logs/\n","import spacy.cli\n","\n","!pip install nltk\n","!pip install -q -U keras-tuner\n","!pip install tensorboard\n","\n","if language == 'german':\n","  spacy.cli.download('de_core_news_md')\n","elif language == 'english':\n","  spacy.cli.download('en_core_web_sm')\n","else:\n","  raise ValueError(\"'language' set to an invalid value!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZV2MIIets6R5"},"source":["Only needed for Google Drive and Colab"]},{"cell_type":"code","metadata":{"id":"jRrVEqH_HfJu"},"source":["if 'google.colab' in str(get_ipython()):\n","  from google.colab import drive\n","  drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SJks0l-pmK6g"},"source":["## 1.1. Imports"]},{"cell_type":"code","metadata":{"id":"g9eYykTQedMP"},"source":["import urllib, IPython, os, datetime, re, nltk, tensorboard, operator, random\n","import tensorflow as tf\n","from tensorflow import keras\n","import tensorflow_hub as hub\n","from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout, BatchNormalization \n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.python.keras.callbacks import TensorBoard\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import tensorflow.keras.activations as activations\n","import tensorflow.keras.losses as losses\n","import tensorflow.keras.optimizers as optimizers\n","import kerastuner as kt\n","from keras.utils.vis_utils import plot_model\n","import matplotlib.pyplot as plt\n","from gensim.models import Word2Vec\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from wordcloud import WordCloud\n","from collections import Counter\n","import spacy\n","from spacy.lemmatizer import Lemmatizer\n","from spacy import displacy\n","\n","\n","if language == 'german':\n","  import de_core_news_md\n","  from spacy.lang.de.stop_words import STOP_WORDS\n","elif language == 'english':\n","  import en_core_web_sm\n","  from spacy.lang.en.stop_words import STOP_WORDS\n","else:\n","  raise ValueError(\"'language' set to an invalid value!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L3mTvrTLECOP"},"source":["## 1.2. Downloads for NLTK and Spacy"]},{"cell_type":"code","metadata":{"id":"6mjK0f0vEAqU"},"source":["nltk.download('punkt')\n","spacy.prefer_gpu()\n","\n","if language == 'german':\n","  nlp = de_core_news_md.load()\n","elif language == 'english':\n","  nlp = en_core_web_sm.load()\n","else:\n","  raise ValueError(\"'language' set to an invalid value!\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WcsMYv9UmvFx"},"source":["## 1.2. Magic functions and global variables"]},{"cell_type":"markdown","metadata":{"id":"dc5mMWU2m3Yd"},"source":["Magic functions"]},{"cell_type":"code","metadata":{"id":"AIVvK2YemWMO"},"source":["%matplotlib inline\n","%load_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N1mB7GhEKyiB"},"source":["### **1.2.1. Set variables and paths** <a class=\"anchor\" id=\"1-2-1\"></a>"]},{"cell_type":"markdown","metadata":{"id":"maH5FORsm6L5"},"source":["Set `session_id` for providing unique file names"]},{"cell_type":"code","metadata":{"id":"dN4kYwXzwGu-"},"source":["session_id = datetime.datetime.now().strftime(\"%d/%m/%Y - %H:%M\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qNVTptxz4O-D"},"source":["**This is the place where some information is needed. Please go though the steps and modify the information according to your needs.**"]},{"cell_type":"markdown","metadata":{"id":"Kb3AYQOrKUG1"},"source":["To free some space after the traing and validation data is created set `auto_free_memory` to `True`."]},{"cell_type":"code","metadata":{"id":"wBZFjDwXK0gZ"},"source":["auto_free_memory = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RhgXsR8F0UQk"},"source":["For tesing the notebook set `test_mode` to `True`. POS tagging (4.2.1.) and lemmatization (6.1.) will **not** be disabled."]},{"cell_type":"code","metadata":{"id":"n6oFpBKE0dce"},"source":["test_mode = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VBTLEKATvTRA"},"source":["Paths used for storing and loading. Should **never** end with `/` or file ending."]},{"cell_type":"code","metadata":{"id":"tT45G48BK-Ce"},"source":["source_path = '/content/drive/My Drive/RUAK/input/processed'\n","dataframe_path = '/content/drive/My Drive/RUAK/output/dataframe'\n","word2vec_path = '/content/drive/My Drive/RUAK/output/embedding/w2v'\n","w2v_model_name = 'full_700_iter100_win7_8'\n","hyperband_tuner_output_path = '/content/drive/My Drive/RUAK/output/hp_tuning'\n","checkpoint_path = '/content/drive/My Drive/RUAK/output/training_checkpoints'\n","model_h5_path = '/content/drive/My Drive/RUAK/output/models'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WdAePzg_nM9i"},"source":["List of files to process and author names. Files should be named after author (e.g. `plato.txt`). `file_names` should contain at least 3 files."]},{"cell_type":"code","metadata":{"id":"NW35U-2oXscx"},"source":["file_names = [\n","    'kant.txt', \n","    'nietzsch.txt', \n","    'platon.txt', \n","    'rousseau.txt']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ofC-NjwR4JAD"},"source":["Parameters needed for tuning and training."]},{"cell_type":"code","metadata":{"id":"9sHv768B348B"},"source":["batch_size=40\n","epochs=30\n","search_epochs=20\n","early_stopping_patience=5\n","executions_per_trial=3\n","hyperband_iterations=3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fBdj3KzTqJXz"},"source":["## 1.3 [Stop words](https://en.wikipedia.org/wiki/Stop_word)"]},{"cell_type":"code","metadata":{"id":"kOifz08hn-KO"},"source":["def replace_umlaut(string):\n","    string = string.replace('ä', 'ae')\n","    string = string.replace('ö', 'oe')\n","    string = string.replace('ü', 'ue')\n","    string = string.replace('Ä', 'Ae')\n","    string = string.replace('Ö', 'Oe')\n","    string = string.replace('Ü', 'Ue')\n","    return string.replace('ß', 'ss')\n","\n","stop_words = set([replace_umlaut(word) for word in STOP_WORDS])\n","print(f'Stop words count: {len(stop_words)}.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y9yIECZO91cG"},"source":["# 2. Loading text data"]},{"cell_type":"code","metadata":{"id":"tgTeoYVTRrw9"},"source":["if len(file_names) < 3:\n","  raise ValueError(\"'file_names' should contain at least 3 files. Add more files at (1.2.1)!\")\n","\n","if test_mode == True:\n","  file_names = file_names[0:3]\n","\n","for file_name in file_names:\n","  text_dir = tf.keras.utils.get_file(file_name, origin=f'file://{source_path}/{file_name}')\n","\n","parent_dir = os.path.dirname(text_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"96g2waSU9t-i"},"source":["# 3. Collect data and create word collections"]},{"cell_type":"markdown","metadata":{"id":"UYahXMunoDtD"},"source":["## 3.1. Prepare word collections"]},{"cell_type":"code","metadata":{"id":"ddeJd1CW1Bxw"},"source":["author_names = [name[:-4].capitalize() for name in file_names]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KeNOOlWpTu71"},"source":["Function to add words to `words_without_stop_words` and `unique_words_without_stop_words`."]},{"cell_type":"code","metadata":{"id":"R7vT1iazMBIU"},"source":["def add_words(sentence):\n","  for word in sentence.split():\n","    word = re.sub(r\"[^a-zA-Z]+\", \"\", word)\n","    if word == '' or len(word) == 1:\n","      continue\n","    if word.lower() not in stop_words:\n","      words_without_stop_words.append(word)\n","      unique_words_without_stop_words.add(word)\n","\n","words_without_stop_words = []\n","unique_words_without_stop_words = set()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8xaLeKDYq5hX"},"source":["## 3.2. Extract sentences"]},{"cell_type":"markdown","metadata":{"id":"wGNi3Yh7mt3y"},"source":["Extract sentences from files and creates labels list. Adjust the language for the `nltk.sent_tokenizer` if needed."]},{"cell_type":"code","metadata":{"id":"gdnFXAtFdYrG"},"source":["labels = []\n","sentences = []\n","\n","for index, file_name in enumerate(file_names):\n","\n","  path = os.path.join(parent_dir, file_name)\n","\n","  with open(path, 'rb') as file: \n","    text = str(file.read())\n","    nltk_sentences = nltk.sent_tokenize(text, language=language)\n","\n","    for sentence in nltk_sentences:\n","      sentence = str(sentence).replace(\"b'\", \"\")\n","      sentences.append(sentence)\n","      labels.append(index)\n","      add_words(sentence)\n","\n","    print(f\"Sentences for {file_name} with label: {index} added.\")\n","\n","print(f'\\n{len(sentences)} sentences found.')\n","print(f'{len(words_without_stop_words)} words found (excl. stop words).')\n","print(f'{len(unique_words_without_stop_words)} unique words found (excl. stop words).')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"awuDoZibMw6p"},"source":["Collect most commen words except the stop words."]},{"cell_type":"code","metadata":{"id":"9CINAlwkMf4M"},"source":["most_common = [word[0] for word in Counter(words_without_stop_words).most_common(20)]\n","most_common_count = {k: v for k, v in Counter(words_without_stop_words).most_common(20)}\n","print('Most common words:')\n","print(most_common)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oVfu1MwGocix"},"source":["## 3.3. Clean data"]},{"cell_type":"code","metadata":{"id":"IDwZJ9EDohfs"},"source":["def short_sentences(length):\n","  short_sentences = [sentence for sentence in sentences if len(sentence.split()) <= length]\n","  print(f'Found {len(short_sentences)} sentences shorter than {length} words.\\n')\n","  return short_sentences\n","\n","def long_sentences(length):\n","  long_sentences = [sentence for sentence in sentences if len(sentence.split()) >= length]\n","  print(f'Found {len(long_sentences)} sentences longer than {length} words.\\n')\n","  return long_sentences"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bhtTOWyarJDd"},"source":["### 3.3.1. Remove sentences"]},{"cell_type":"markdown","metadata":{"id":"3n3i-snVro6O"},"source":["Set min and max length for sentences"]},{"cell_type":"code","metadata":{"id":"VRuSh6_SrSSj"},"source":["min_length = 6\n","max_length = 400"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JYAlG39IsLHv"},"source":["Get invalid sentences"]},{"cell_type":"code","metadata":{"id":"A9z6pG3Fryce"},"source":["invalid_sentences = short_sentences(min_length) + long_sentences(max_length)\n","print(f'Found {len(invalid_sentences)} invalid sentences.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lhm1YKpXtGe5"},"source":["#### 3.3.1.1. Investigate invalid sentences\n","Print 5 examples of `invalid_sentences`"]},{"cell_type":"code","metadata":{"id":"bhPflHoDtLyp"},"source":["for i in random.sample(range(10, len(invalid_sentences)-1), 10):\n","  print(invalid_sentences[i])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HpemuuR72jxm"},"source":["Use Spacy [Visualizer](https://spacy.io/usage/visualizers) to show a random invalid sentence."]},{"cell_type":"code","metadata":{"id":"4oDAA_zi1cFm"},"source":["if tf.test.gpu_device_name() == '': # Needed to avoid chash in Spacy when running a GPU.\n","  doc = nlp(invalid_sentences[random.randint(0, len(invalid_sentences)-1)])\n","  displacy.render(doc, style=\"dep\", jupyter=True, options={'compact':'True'})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nTg_hLTivcyj"},"source":["### 3.3.2. Provide cleaned data"]},{"cell_type":"code","metadata":{"id":"GlUS_PuEvnvM"},"source":["cleaned_labels = []\n","cleaned_sentences = []\n","print(f\"'sentences' list length before removal: {len(sentences)}.\")\n","for index, sentence in enumerate(sentences):\n","  if sentence not in invalid_sentences:\n","    cleaned_sentences.append(sentence)\n","    cleaned_labels.append(labels[index])  \n","print(f\"'sentences' list length after removal: {len(cleaned_sentences)}.\")\n","print(f'{len(invalid_sentences)} sentences removed.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"muvXsp-L2RjH"},"source":["# 4. Create and extend DataFrame\n","Some helper methods"]},{"cell_type":"code","metadata":{"id":"10sfumjtrcsw"},"source":["def stop_word_ratio_fn(sentence):\n","  count = 0\n","  for word in sentence.split():\n","    word = re.sub(r\"[^a-zA-Z]+\", \"\", word)\n","    if word.lower() in stop_words:\n","      count += 1\n","  return round(count/len(sentence.split()) * 100, 2)\n","\n","def stop_word_count_fn(sentence):\n","  count = 0\n","  for word in sentence.split():\n","    word = re.sub(r\"[^a-zA-Z]+\", \"\", word)\n","    if word.lower() in stop_words:\n","      count += 1\n","  return count\n","\n","def mean_word_length_fn(sentence):\n","  return round(np.array([len(word) for word in sentence.replace('.','').split()]).mean(), 2)\n","\n","def pos_count(sentence, pos):\n","  doc = nlp(sentence)\n","  return len([w.pos_ for w in doc if w.pos_ == pos])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZxICrBEKbZE1"},"source":["## 4.1. Create DataFrame"]},{"cell_type":"code","metadata":{"id":"uFs6VfqrbQzg"},"source":["df = pd.DataFrame({'label': cleaned_labels, 'sentence': cleaned_sentences})\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8rsP0kemG7wE"},"source":["Remove 90% of the rows for `test_mode`"]},{"cell_type":"code","metadata":{"id":"K6jm_w6yHBFM"},"source":["if test_mode == True:\n","  print(f'Before drop: {df.shape}')\n","  df = df.drop(df.sample(frac=0.9).index)\n","  print(f'After drop: {df.shape}')\n","else:\n","  print('Test mode not enabled - nothing dropped.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XtHcxdysQuPe"},"source":["## 4.2. Construct new data"]},{"cell_type":"code","metadata":{"id":"qql42ncCQtK5"},"source":["df['author'] = df['label'].map(lambda x: author_names[x])\n","df['word_count'] = df['sentence'].str.split().str.len()\n","df['mean_word_length'] = df['sentence'].map(mean_word_length_fn)\n","df['stop_words_ratio'] = df['sentence'].map(stop_word_ratio_fn)\n","df['stop_words_count'] = df['sentence'].map(stop_word_count_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j-PMNxYGUHj_"},"source":["### 4.2.1. POS tagging <a class=\"anchor\" id=\"4-2-*1*\"></a>\n","Add columns and values for [POS tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging).A list of tags can be found [here](https://spacy.io/api/annotation). **This may take a while!**"]},{"cell_type":"code","metadata":{"id":"KwCwoFniz8l8"},"source":["pos_tagging_enabled = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oaAHSANZUG9J"},"source":["if pos_tagging_enabled == True:\n","  pos_tags = ['ADJ', 'ADV', 'ADP', 'AUX', 'DET', 'NUM', 'X', 'INTJ', 'CONJ',\n","              'CCONJ', 'SCONJ', 'PROPN', 'NOUN', 'PRON', 'PART', 'VERB']\n","            \n","  for tag in pos_tags:\n","    df[f'{tag}_count'] = df['sentence'].map(lambda sen: pos_count(sen, tag))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4JYzLWqkrh_Y"},"source":["## 4.3. Preview processed DataFrame"]},{"cell_type":"code","metadata":{"id":"7mXU1V4GazIh"},"source":["df.head(df.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1t6t2UGRo1Jg"},"source":["# 5. Store or load DataFrame"]},{"cell_type":"markdown","metadata":{"id":"PYrSJXlMn5Q_"},"source":["Save DataFrame to CSV if needed."]},{"cell_type":"code","metadata":{"id":"yUD-BM87n9tJ"},"source":["if test_mode == False:\n","  file_path = f'{dataframe_path}/ruak_dataframe.csv'\n","else: \n","  file_path = f'{dataframe_path}/ruak_dataframe_testing.csv'  \n","\n","df.to_csv(file_path, index=False)\n","print(f'DataFrame saved to: {file_path}.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FtFcnhfko9wQ"},"source":["Load the DataFrame from CSV if needed."]},{"cell_type":"code","metadata":{"id":"naVqKAOXpDoL"},"source":["if test_mode == False:\n","  file_path = f'{dataframe_path}/ruak_dataframe.csv'\n","else: \n","  file_path = f'{dataframe_path}/ruak_dataframe_testing.csv'  \n","\n","df = pd.read_csv(file_path)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Fo0XiX7S6dr"},"source":["# 6. Visualization of data"]},{"cell_type":"markdown","metadata":{"id":"DkQFhI6J0Sw_"},"source":["## 6.1 Prepare values for visualization"]},{"cell_type":"markdown","metadata":{"id":"Dg-2j4xO_Ua7"},"source":["Count vocabulary"]},{"cell_type":"code","metadata":{"id":"GpYhWH3Njg0h"},"source":["def vocabulary_count_fn(series, lemmatization):\n","  vocabulary = set()\n","  for sentence in series:\n","    if lemmatization == True:\n","      words = lemmatize(sentence)\n","    else:\n","      words = sentence.split()  \n","    for word in words:\n","      if word.lower() not in stop_words:\n","        word = re.sub(r'[^a-zA-Z]+', '', word)\n","        vocabulary.add(word.lower())\n","  return len(vocabulary)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"63Nrf9EI1ugk"},"source":["[Lemmatize](https://en.wikipedia.org/wiki/Lemmatisation)"]},{"cell_type":"code","metadata":{"id":"COZcha201wQP"},"source":["def lemmatize(sentence):\n","  words = set()\n","  doc = nlp(sentence)\n","  for word in doc:\n","    words.add(word.lemma_)\n","  return list(words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4JYXioUBSsAu"},"source":["### 6.1.1. Prepare values for visualization <a class=\"anchor\" id=\"6-1-1\"></a>"]},{"cell_type":"code","metadata":{"id":"PdHeUoJdi5Gp"},"source":["lemmatization_enabled = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CH14xXHg_i0c"},"source":["Prepare values for visualization. Enable lemmatization to get more a more prezise `unique_vocabulary_count`. **This will slow down the process!**"]},{"cell_type":"code","metadata":{"id":"XieassmAz9fb"},"source":["median_sentence_length = df.groupby('label')['word_count'].median()\n","median_stop_words = df.groupby('label')['stop_words_ratio'].median()\n","sentence_count = df.groupby('label')['sentence'].count()\n","unique_vocabulary_count = df.groupby('label')['sentence'].apply(lambda ser: vocabulary_count_fn(ser, lemmatization_enabled))\n","\n","if 'ADJ_count' in df: # Checks if dataframe contain POS tagging information.\n","  pos_df = df.groupby('author')[[f'{tag}_count' for tag in pos_tags]+['word_count']] \\\n","  .sum().apply((lambda x: x/x['word_count']*100), axis=1) \\\n","  .drop('word_count', axis=1)\n","\n","  pos_tags = ['ADJ', 'ADV', 'ADP', 'AUX', 'DET', 'NUM', 'X', 'INTJ', 'CONJ', \n","              'CCONJ', 'SCONJ', 'PROPN', 'NOUN', 'PRON', 'PART', 'VERB']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z3e6981g09wl"},"source":["## 6.2. Draw visualization"]},{"cell_type":"markdown","metadata":{"id":"wNVDqenRPHpg"},"source":["### 6.2.1 Data distribution\n","The data should be equally split between authors."]},{"cell_type":"code","metadata":{"id":"Ch_70OxpOoYG"},"source":["plt.pie(df['author'].value_counts(),\n","        explode=np.full(len(author_names), 0.1),\n","        radius=2,\n","        autopct='%1.0f%%', \n","        labels=author_names,\n","        shadow=True,\n","        startangle=90,\n","        textprops={'size': 15})\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6TRUVgAr_7j9"},"source":["### 6.2.2. Comparing authors"]},{"cell_type":"code","metadata":{"id":"oJkpwQ6IUyKD"},"source":["fig, axs = plt.subplots(4,1, figsize=(10,15))\n","fig.tight_layout(h_pad=6)\n","\n","axs[0].bar(author_names, sentence_count)\n","axs[0].set_ylabel('Number of sentences', fontdict={'color':'gray', 'size':12})\n","axs[0].tick_params(axis='both', colors='gray', labelsize=12)\n","axs[0].grid()\n","\n","axs[1].bar(author_names, median_sentence_length)\n","axs[1].set_ylabel('Median sentence lenth', fontdict={'color':'gray', 'size':12})\n","axs[1].tick_params(axis='both', colors='gray', labelsize=12)\n","axs[1].grid()\n","\n","axs[2].bar(author_names, unique_vocabulary_count)\n","axs[2].set_ylabel('Unique vocabulary count\\n(excl. stop words)', fontdict={'color':'gray', 'size':12})\n","axs[2].tick_params(axis='both', colors='gray', labelsize=12)\n","axs[2].grid()\n","\n","axs[3].bar(author_names, median_stop_words)\n","axs[3].set_ylabel('Median stop words ratio', fontdict={'color':'gray', 'size':12})\n","axs[3].tick_params(axis='both', colors='gray', labelsize=12)\n","axs[3].grid()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7emYOcFsgk8o"},"source":["Word classes by authors. A list of tags can be found [here](https://spacy.io/api/annotation)."]},{"cell_type":"code","metadata":{"id":"yC1ukv4Kx5qF"},"source":["if 'pos_df' in locals() and 'pos_tags' in locals():\n","  ax = pos_df.plot(kind='barh', figsize=(15,15))\n","  ax.set_xlabel('Percentage of word class in vocabulary', fontdict={'color':'gray', 'size':17})\n","  ax.set_ylabel('Author', fontdict={'color':'gray', 'size':17})\n","  ax.tick_params(axis='both', colors='gray', labelsize=17)\n","  ax.legend(pos_tags)\n","  ax.xaxis.set_tick_params(labeltop='on')\n","  ax.grid()\n","  plt.show()\n","else:\n","  print(\"'pod_df' or 'pos_tags' not available!\") "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QdmHHaPJ_zdx"},"source":["### 6.2.3. Common words"]},{"cell_type":"code","metadata":{"id":"yUqVaJBpR8mW"},"source":["fig, axs = plt.subplots(1,1, figsize=(30,10))\n","\n","axs.bar(most_common_count.keys(), most_common_count.values(), color='orange')\n","axs.set_ylabel('Number of sentences', fontdict={'color':'gray', 'size':17})\n","axs.tick_params(axis='both', colors='gray', labelsize=17)\n","axs.grid()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NhWNIAP2azJH"},"source":["### 6.2.4. Sentences by authors\n","This shows the sentence structure, lemmas, and pos tags of one random sentences from each author."]},{"cell_type":"code","metadata":{"id":"cAo74rFzaz6L"},"source":["if tf.test.gpu_device_name() == '': # Needed to avoid chash in Spacy when running a GPU.\n","  for author in author_names:\n","    sentences_series = df.loc[(df['author'] == author) & (df['sentence'].str.len() < 50)]['sentence']\n","    print(f'\\nSentence by {author}:')\n","    doc = nlp(sentences_series.sample(n=1).values[0])\n","    displacy.render(doc, \n","                    style=\"dep\", \n","                    jupyter=True, \n","                    options={'compact':'True', 'add_lemma': 'True'})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8YEfw66S_oVo"},"source":["### 6.2.5. Word cloud"]},{"cell_type":"code","metadata":{"id":"QmPbGhTn5tHy"},"source":["wordcloud = WordCloud(width=5000, \n","                      height=4000,\n","                      max_words=20,  \n","                      background_color ='black', \n","                      stopwords = stop_words, \n","                      min_font_size = 10).generate_from_frequencies(most_common_count) \n","\n","plt.figure(figsize=(20, 12), facecolor='k', edgecolor ='k') \n","plt.imshow(wordcloud) \n","plt.axis(\"off\") \n","plt.tight_layout(pad=0) \n","plt.show() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"61D3POnhr3fT"},"source":["# 7. Prepare and split"]},{"cell_type":"markdown","metadata":{"id":"Aad2fR1Rs5bn"},"source":["## 7.1. Tokenize"]},{"cell_type":"code","metadata":{"id":"TKA1Om5OtNFd"},"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df['sentence'].values)\n","print(f\"{len(df['sentence'].values)} sentences from {len(file_names)} authors.\")\n","print(f'{len(tokenizer.word_counts)} unique vocabularies.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JwIYTz819jIk"},"source":["## 7.2. Encode"]},{"cell_type":"code","metadata":{"id":"DaLmBJfP73gJ"},"source":["encoded_sentences = tokenizer.texts_to_sequences(df['sentence'].values)\n","padded_sentences = pad_sequences(encoded_sentences, padding='post')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s5mvazkEy4pF"},"source":["Test the encoder"]},{"cell_type":"code","metadata":{"id":"ZeS-BVU9y6d7"},"source":["print(df['sentence'].values[0])\n","print(np.array(padded_sentences[0]))\n","print(tokenizer.sequences_to_texts([padded_sentences[0]]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dZMGHboj8yRE"},"source":["## 7.3. Splitting\n","Create train and test data for the fitting proccess."]},{"cell_type":"code","metadata":{"id":"5uHhk9JCoyXD"},"source":["X_train, X_valid, y_train, y_valid = train_test_split(padded_sentences, df['label'].values, test_size=0.1)\n","print(f'Shape of the splited X_train: {X_train.shape}')\n","print(f'Shape of the splited y_train: {y_train.shape}')\n","print(f'Shape of the splited X_valid: {X_valid.shape}')\n","print(f'Shape of the splited y_valid: {y_valid.shape}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PVVYN4t5kJU2"},"source":["# 8. Hyperparameter tuning"]},{"cell_type":"markdown","metadata":{"id":"WR3UNPa4Vetw"},"source":["Free some space"]},{"cell_type":"code","metadata":{"id":"CnQJvyIsVfGQ"},"source":["if auto_free_memory == True:\n","  del df\n","  del wordcloud\n","  del encoded_sentences\n","  del sentences\n","  del labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xur6_r2MJ2QE"},"source":["## 8.1. Setup the hypermodel"]},{"cell_type":"markdown","metadata":{"id":"wQOGHaIQe4sZ"},"source":["### 8.1.1. Load the Word2Vec model\n","For german the custom Word2Vec layer is used."]},{"cell_type":"code","metadata":{"id":"P7g_As6hpxRe"},"source":["def embedding_matrix_custom_model():\n","    model = Word2Vec.load(f'{word2vec_path}/{w2v_model_name}.model')\n","    embedding_matrix = np.zeros((len(model.wv.vocab), model.vector_size))\n","    for i in range(len(model.wv.vocab)):\n","        embedding_vector = model.wv[model.wv.index2word[i]]\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","    return embedding_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pjucNYdZp8e0"},"source":["For English we use Wiki-words-500 provided by Tensorflow Hub."]},{"cell_type":"code","metadata":{"id":"PiC0yHwspxA_"},"source":["def embedding_matrix_hub_model():\n","    hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/Wiki-words-500/2\", input_shape=[], dtype=tf.string)\n","    return hub_layer.get_weights()[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Y9YgOClCcyD"},"source":["if language == 'german':\n","  embedding_matrix = embedding_matrix_custom_model()\n","elif language == 'english':\n","  embedding_matrix = embedding_matrix_hub_model()\n","else:\n","  raise ValueError(\"'language' set to an invalid value!\")\n","\n","print(f'Embedding_matrix shape: {embedding_matrix.shape}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"asCVxLd4lbRF"},"source":["embedding_matrix.shape[1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x0Y5OklrkvWh"},"source":["hub_layer.get_weights()[0].shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZU_5ktAbtXUz"},"source":["###8.1.2. Define the hypermodel"]},{"cell_type":"code","metadata":{"id":"i5uA91DTI6I0"},"source":["def hypermodel(hp):\n","\n","  if test_mode == True:\n","    hp_dense_count = hp.Int('dense_count', min_value=1, max_value=2, step=1)\n","    hp_embedding_trainable = hp.Choice('embedding_trainable', [False])\n","    hp_with_batch_normalization = hp.Choice('with_batch_normalization', [True])\n","    hp_lstm_units = hp.Int('lstm_units', 32, 64, step=32)\n","    hp_dropout = hp.Choice('dropout', [0.25])\n","    hp_learning_rate = hp.Choice('learning_rate', [0.001])\n","    hp_adam_epsilon = hp.Choice('adam_epsilon', values=[1e-08])\n","  else:\n","    hp_dense_count = hp.Int('dense_count', min_value=1, max_value=7, step=1)\n","    hp_embedding_trainable = hp.Choice('embedding_trainable', [True, False])\n","    hp_with_batch_normalization = hp.Choice('with_batch_normalization', [True, False])\n","    hp_lstm_units = hp.Int('lstm_units', 256, 512, step=128)\n","    hp_dropout = hp.Choice('dropout', [0.0, 0.1, 0.25, 0.5])\n","    hp_learning_rate = hp.Choice('learning_rate', [0.01, 0.001, 0.0001])\n","    hp_adam_epsilon = hp.Choice('adam_epsilon', values=[1e-07, 1e-08])\n","\n","  model = tf.keras.Sequential()\n","\n","  model.add(Embedding(len(embedding_matrix),\n","            output_dim=embedding_matrix.shape[1],\n","            weights=[embedding_matrix], \n","            trainable=hp_embedding_trainable,\n","            mask_zero=True))\n","\n","  model.add(Bidirectional(LSTM(hp_lstm_units, return_sequences=True)))\n","  if hp_embedding_trainable == True:\n","    BatchNormalization()\n","  model.add(Dropout(hp_dropout))\n","\n","  model.add(Bidirectional(LSTM(hp_lstm_units, return_sequences=True)))\n","  if hp_embedding_trainable == True:\n","    BatchNormalization()  \n","  model.add(Dropout(hp_dropout))\n","\n","  model.add(Bidirectional(LSTM(hp_lstm_units, return_sequences=False)))\n","  if hp_embedding_trainable == True:\n","    BatchNormalization()  \n","  model.add(Dropout(hp_dropout))\n","\n","  for i in range(hp_dense_count):\n","\n","    if test_mode == True:\n","      hp_dense_units = hp.Int(f'dense_units{i}', 64, 128, step=64)\n","      hp_dense_activation = hp.Choice(f'dense_activation_{i}', values=['relu'])\n","    else: \n","      hp_dense_units = hp.Int(f'dense_units{i}', 64, 512, step=64)\n","      hp_dense_activation = hp.Choice(f'dense_activation_{i}', values=['tanh', 'relu'])\n","\n","    model.add(Dense(hp_dense_units, activation=hp_dense_activation))\n","\n","  model.add(Dense(len(file_names), activation='softmax'))\n","\n","  model.compile(optimizer=optimizers.Adam(learning_rate=hp_learning_rate, epsilon=hp_adam_epsilon),\n","              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2v8MXrJrwRmp"},"source":["## 8.2. Run the tuner\n","Reduce parameters for `testing_mode`"]},{"cell_type":"code","metadata":{"id":"uJOuo19A16Mh"},"source":["if test_mode == True:\n","  epochs=4\n","  search_epochs=1\n","  early_stopping_patience=4\n","  executions_per_trial=1\n","  hyperband_iterations=1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GbHsa2qdUKxb"},"source":["Set variables"]},{"cell_type":"code","metadata":{"id":"eeoGHkEgUNjs"},"source":["max_epochs = epochs+5\n","project_name = 'RUAK'\n","verbose = 2\n","if test_mode == True:\n","  max_epochs = 1\n","  project_name = 'RUAK_testing'\n","  verbose = 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W9Gd2vHVRvNw"},"source":["Prepare the hyperband tuner"]},{"cell_type":"code","metadata":{"id":"PvrDPbJsMN7A"},"source":["tuner = kt.Hyperband(hypermodel,\n","                     objective='val_accuracy', \n","                     executions_per_trial=executions_per_trial,\n","                     max_epochs=max_epochs,\n","                     hyperband_iterations=hyperband_iterations,\n","                     directory=hyperband_tuner_output_path,\n","                     project_name=project_name,\n","                     overwrite=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lr7vVHUiT4lE"},"source":["Run the tuner to search for best parameters. The result are the optimal hyperparameters: `best_hps` and a list of `best_models`."]},{"cell_type":"code","metadata":{"id":"9raFi0ERTp5E"},"source":["class ClearTrainingOutput(tf.keras.callbacks.Callback):\n","  def on_train_end(*args, **kwargs):\n","    IPython.display.clear_output(wait=True)\n","\n","tuner.search(X_train, y_train, \n","             epochs=search_epochs,\n","             validation_data = (X_valid, y_valid),\n","             callbacks = [ClearTrainingOutput(), EarlyStopping('val_accuracy', patience=1)],\n","             verbose=verbose)\n","\n","best_hps = tuner.get_best_hyperparameters(1)[0]\n","best_models = tuner.get_best_models(num_models=3)\n","\n","tuner.results_summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u2PdigVLCdcL"},"source":["# 9. Model preparation and training"]},{"cell_type":"markdown","metadata":{"id":"T0RQAk0PbU_X"},"source":["Get summaries of the best models and choose the model for training."]},{"cell_type":"code","metadata":{"id":"PwVKYs2XzpUp"},"source":["for model in best_models:\n","  model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gyx6tUlnzK1M"},"source":["Choose preferred model"]},{"cell_type":"code","metadata":{"id":"zUMY2E68bUZ-"},"source":["chosen_model = best_models[0]\n","del best_models"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-_Tur-i_y5fB"},"source":["Plot model structure"]},{"cell_type":"code","metadata":{"id":"_c0wvzcRy4xO"},"source":["plot_model(chosen_model, show_shapes=True, show_layer_names=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hz9f9MAqhg6c"},"source":["## 9.1. Prepare callbacks\n","TensorBoard preparation"]},{"cell_type":"code","metadata":{"id":"oX7suDrVRiYq"},"source":["log_dir = os.path.join('logs', session_id)\n","tb_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_E6Aba9xuinl"},"source":["Create `ModelCheckpoint` and `EarlyStopping` callbacks."]},{"cell_type":"code","metadata":{"id":"mCZgMlv-hkaU"},"source":["cp_callback = ModelCheckpoint(filepath=f'{checkpoint_path}/cp.ckpt',\n","                                                 save_weights_only=True,\n","                                                 verbose=2)\n","\n","es_callback = EarlyStopping('val_accuracy', patience=early_stopping_patience, restore_best_weights=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S4U6I-8lCivy"},"source":["## 9.2. Model training"]},{"cell_type":"code","metadata":{"id":"6gksI7UJCce6"},"source":["callbacks = [cp_callback, es_callback, tb_callback]\n","if test_mode == False:\n","    callbacks = [es_callback, tb_callback]\n","    print('Running in test mode!')\n"," \n","h = chosen_model.fit(X_train, \n","                      y_train, \n","                      epochs=epochs, \n","                      batch_size=batch_size, \n","                      validation_data=(X_valid, y_valid), \n","                      callbacks=callbacks,\n","                      verbose=verbose)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KpOp6znCCCpu"},"source":["# 10. Save or load model"]},{"cell_type":"markdown","metadata":{"id":"AYJ4k-VLKHdo"},"source":["## 10.1. Save model"]},{"cell_type":"code","metadata":{"id":"y4TgjSFvKG7c"},"source":["if test_mode == False:\n","  model.save(f'{model_h5_path}/ruak_model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dua2WMwGCHFd"},"source":["## 10.2. Load model"]},{"cell_type":"code","metadata":{"id":"iAh4a9qiCFew"},"source":["if test_mode == False:\n","  chosen_model = tf.keras.models.load_model(f'{model_h5_path}/ruak_model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4jQajX37z3bO"},"source":["## 10.3. Load weights"]},{"cell_type":"code","metadata":{"id":"vwyOI709_Tu6"},"source":["if test_mode == False:\n","  latest = tf.train.latest_checkpoint(f'{checkpoint_path}/cp.ckpt')\n","  chosen_model.load_weights(latest)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uHHVXU4C7nc9"},"source":["# 11. Evaluation"]},{"cell_type":"markdown","metadata":{"id":"WA-xq__FeVJg"},"source":["Draw charts to show compare training and validation results"]},{"cell_type":"code","metadata":{"id":"k0_FQ4v3X3iz"},"source":["fig, axs = plt.subplots(2,1, figsize=(8, 6))\n","\n","epochs = range(len(h.history['accuracy']))\n","axs[0].plot(epochs, h.history['accuracy'], color='red', marker='x')\n","axs[0].plot(epochs, h.history['val_accuracy'], color='green', marker='.')\n","axs[0].legend(labels=['Training accuracy','Validation accuracy'])\n","axs[0].set_ylabel('Accuracy', fontdict={'color':'gray', 'size':12})\n","axs[0].tick_params(labelbottom=False)\n","axs[0].grid()\n","\n","epochs = range(len(h.history['loss']))\n","axs[1].plot(epochs, h.history['loss'], color='red', marker='x')\n","axs[1].plot(epochs, h.history['val_loss'], color='green', marker='.')\n","axs[1].legend(labels=['Training loss','Validation loss'])\n","axs[1].set_ylabel('Loss', fontdict={'color':'gray', 'size':12})\n","axs[1].tick_params(labelbottom=False)\n","axs[1].grid()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9UwQPCjCvscA"},"source":["Show loss and accuracy"]},{"cell_type":"code","metadata":{"id":"1yB-R-XK7rEx"},"source":["val_loss, val_acc = chosen_model.evaluate(X_valid)\n","\n","print(f'Validation Accuracy: {val_acc}')\n","print(f'Validation Loss: {val_loss}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RiHTWqlnv3Wi"},"source":["## 11.1. Test the model"]},{"cell_type":"markdown","metadata":{"id":"UAv-WNEmMxti"},"source":["Test the model. Add  `sample_sentences` to get the probability distribution for each author."]},{"cell_type":"code","metadata":{"id":"BpLozmZOGU7I"},"source":["sample_sentences = [\n","                    # 0 - Platon\n","                    \"Das ziemt uns ja auch nicht, Sokrates\",\n","                    # 1 - Nietzsche\n","                    \"noch hat er seine that nicht ueberwunden.\", \n","                    # 2 - Kant\n","                    \"Die Grenzen der Ausdehnung bestimmen die Figur.\"\n","                   ]\n","\n","# Add your own sentences:\n","# sample_sentences = [\"Here you can try your own sentences.\", \"Let's see what kind of philosopher you are.\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VhElQ-nKN7em"},"source":["encoded_sample_sentences = tokenizer.texts_to_sequences(sample_sentences)\n","padded_sample_sentences = pad_sequences(encoded_sample_sentences, maxlen=X_train.shape[1], padding='post')\n","predictions = model.predict(padded_sample_sentences)\n","\n","predictions_df = pd.DataFrame()\n","for index, prediction in enumerate(predictions):\n","  for i, pre in enumerate(prediction):\n","    predictions_df = predictions_df.append({\n","      'sentence_number': index,\n","      'author': author_names[i],\n","      'prediction': pre,\n","      'sentence': sample_sentences[index]\n","    }, ignore_index=True)\n","\n","predictions_df.head(predictions_df.shape[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EwMK3z6r4CQh"},"source":["Draw bars for each sample sentence"]},{"cell_type":"code","metadata":{"id":"Tq8-cebk4BaH"},"source":["fig, axs = plt.subplots(len(predictions), 1, figsize=(10,10))\n","fig.tight_layout(h_pad=6)\n","for sen_id, pre in enumerate(predictions):\n","  for i, p in enumerate(pre):\n","    axs[sen_id].barh(author_names, pre)\n","    axs[sen_id].set_title(f'Sentence {sen_id}: {sample_sentences[sen_id][0:70]}...', \n","                          fontdict={'color':'gray', 'size':12, 'fontweight':'bold'})\n","    axs[sen_id].set_ylabel('Author', fontdict={'color':'gray', 'size':12})\n","    axs[sen_id].set_xlabel('Probability', fontdict={'color':'gray', 'size':12})\n","    \n","    axs[sen_id].tick_params(axis='both', colors='gray', labelsize=12)\n","    axs[sen_id].grid()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l5TqibMKjSmM"},"source":["# 12. TensorBoard <a class=\"anchor\" id=\"12\"></a>"]},{"cell_type":"code","metadata":{"id":"8oJRvGKH7KmT"},"source":["%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JxG2o_axywgE"},"source":["Clean TensorBoad logs"]},{"cell_type":"code","metadata":{"id":"0o8W7ogaywG0"},"source":["!rm -rf ./logs/"],"execution_count":null,"outputs":[]}]}