{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"philo_text_classification.ipynb","provenance":[{"file_id":"1-GLcGn8_wvQsyMD6VCOHgtZ_Zsw3eqsr","timestamp":1589599796682}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"DmSWDUvClRyu"},"source":["!pip install tensorflow_datasets\n","!pip install -q -U keras-tuner\n","!pip install -q pyyaml h5py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jRrVEqH_HfJu"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g9eYykTQedMP"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import tensorflow_datasets as tfds\n","import tensorboard\n","from tensorflow.python.keras.callbacks import TensorBoard\n","import urllib\n","import os\n","import datetime\n","import re\n","import nltk\n","import IPython\n","import kerastuner as kt\n","import tensorflow.keras.activations as activations\n","import tensorflow.keras.losses as losses\n","import tensorflow.keras.optimizers as optimizers\n","from gensim.models import Word2Vec\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dN4kYwXzwGu-"},"source":["SESSION_ID = datetime.datetime.now().strftime(\"%d%m%Y-%H%M\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FzJEXoSrfTJm"},"source":["Add the file names which should be processed"]},{"cell_type":"code","metadata":{"id":"NW35U-2oXscx"},"source":["FILE_NAMES = [\n","    'pg_kant.txt', \n","    'pg_nietzsch.txt', \n","    'pg_platon.txt', \n","    'pg_rousseau.txt']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y9yIECZO91cG"},"source":["# Preparation / normalization"]},{"cell_type":"markdown","metadata":{"id":"x4TPXvl1gYS4"},"source":["### Loading files with Keras"]},{"cell_type":"code","metadata":{"id":"tgTeoYVTRrw9"},"source":["prefix = 'file://'\n","processed_path = '/content/drive/My Drive/RUAK/input/processed/'\n","url = urllib.parse.quote(processed_path)\n","\n","for file_name in FILE_NAMES:\n","  text_dir = tf.keras.utils.get_file(file_name, origin=prefix+url+file_name)\n","\n","parent_dir = os.path.dirname(text_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"96g2waSU9t-i"},"source":["# Dataset managment"]},{"cell_type":"markdown","metadata":{"id":"wGNi3Yh7mt3y"},"source":["Create datasets - a seperate one for each text"]},{"cell_type":"code","metadata":{"id":"gdnFXAtFdYrG"},"source":["nltk.download('punkt')\n","\n","def labeler(example, index):\n","  return example, tf.cast(index, tf.int64)\n","\n","def to_sentences(text):\n","  return nltk.sent_tokenize(text, language='german')\n","\n","\n","labeled_data_sets = []\n","\n","for index, file_name in enumerate(FILE_NAMES):\n","  path = os.path.join(parent_dir, file_name)\n","  tensor = tf.io.read_file(path)\n","\n","  tensors = []\n","\n","  with open(path, 'rb') as file: \n","    text = str(file.read())\n","    sentences = to_sentences(text)\n","\n","    # Some cleanup for short sentences.\n","    for sentence in sentences:\n","      if ' ' in sentence == False:\n","        continue\n","      if len(sentence) <=20:\n","        continue\n","      tensors.append(tf.constant(sentence))\n","\n","    dataset = tf.data.Dataset.from_tensor_slices(tensors)\n","\n","    labeled_dataset = dataset.map(lambda ex: labeler(ex, index))\n","    labeled_data_sets.append(labeled_dataset)\n","\n","    print(f\"Created dataset for {file_name} with index: {index}.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EPF_TOFeSG8i"},"source":["Define some values. If this in only used for hyperparameter tuning. The fist case should be used."]},{"cell_type":"code","metadata":{"id":"tVWCIVgrSU-r"},"source":["buffer_size = 87710\n","batch_size = 40"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BSvPzaT8acEd"},"source":["Combine the labeled datasets into a single dataset"]},{"cell_type":"code","metadata":{"id":"8LA7YUZqepk_"},"source":["all_labeled_data = labeled_data_sets[0]\n","for labeled_dataset in labeled_data_sets[1:]:\n","  all_labeled_data = all_labeled_data.concatenate(labeled_dataset)\n","\n","all_labeled_data = all_labeled_data.shuffle(\n","    buffer_size, reshuffle_each_iteration=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"61D3POnhr3fT"},"source":["# Tokenization"]},{"cell_type":"code","metadata":{"id":"juFOaq0kvH9e"},"source":["tokenizer = tfds.deprecated.text.Tokenizer(alphanum_only=True) # TODO: Change tokenizer due to deprecation.\n","\n","vocabulary_set = set()\n","sentences_count = 0\n","\n","for sentence_tensor, _ in all_labeled_data:\n","  sentences_count += 1\n","  some_tokens = tokenizer.tokenize(sentence_tensor.numpy())\n","  lower_tokens = []\n","  for token in some_tokens:\n","    lower_tokens.append(token)\n","\n","  vocabulary_set.update(lower_tokens)\n","\n","vocab_size = len(vocabulary_set)\n","print(f'{sentences_count} sentences from {len(FILE_NAMES)} authors.')\n","print(f'{vocab_size} unique vocabularies.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JwIYTz819jIk"},"source":["# Encoding"]},{"cell_type":"code","metadata":{"id":"iQ1rTiU10i9q"},"source":["encoder = tfds.deprecated.text.TokenTextEncoder(vocabulary_set, lowercase=False, \n","                                                strip_vocab=True) # TODO: Change encoder due to deprecation.\n","\n","\n","def encode_text(text_tensor, label):\n","  encoded_text = encoder.encode(text_tensor.numpy())\n","  return encoded_text, label\n","\n","def encode_map(text, label):\n","  encoded_text, label = tf.py_function(encode_text, \n","                                       inp=[text, label], \n","                                       Tout=(tf.int64, tf.int64))\n","\n","  encoded_text.set_shape([None])\n","  label.set_shape([])\n","  return encoded_text, label\n","\n","all_encoded_dataset = all_labeled_data.map(encode_map)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jFnRIshdmlpx"},"source":["Check encoding process"]},{"cell_type":"code","metadata":{"id":"toTagamXi6Yb"},"source":["for sentence, index in all_labeled_data.take(1):\n","  print(sentence.numpy())\n","\n","for encoded_sentence, index in all_encoded_dataset.take(1):\n","  print(encoded_sentence.numpy())\n","  decode_sample_text = encoder.decode(encoded_sentence.numpy())\n","  print(decode_sample_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dZMGHboj8yRE"},"source":["# Splitting"]},{"cell_type":"markdown","metadata":{"id":"1nZjfiMr-QLV"},"source":["Create train and test data for the fitting proccess."]},{"cell_type":"code","metadata":{"id":"42cwkF_D81sy"},"source":["take_size = int(sentences_count * 0.2)\n","\n","train_data = all_encoded_dataset.skip(take_size)\n","train_data = train_data.shuffle(buffer_size)\n","\n","train_data = train_data.padded_batch(batch_size)\n","\n","test_data = all_encoded_dataset.take(take_size) \n","test_data = test_data.shuffle(buffer_size)\n","\n","test_data = test_data.padded_batch(batch_size) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0UMTts6bBDGk"},"source":["Check batching process"]},{"cell_type":"code","metadata":{"id":"qn60xm5vbkYQ"},"source":["for batch, i in train_data.take(1):\n","  print(i)\n","  print(batch)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PVVYN4t5kJU2"},"source":["# Hyperparameter tuning"]},{"cell_type":"markdown","metadata":{"id":"xur6_r2MJ2QE"},"source":["### setup the test model"]},{"cell_type":"markdown","metadata":{"id":"ZU_5ktAbtXUz"},"source":["Hyperparameter Tuning"]},{"cell_type":"code","metadata":{"id":"i5uA91DTI6I0"},"source":["def model_builder(hp):\n","\n","  hp_units = hp.Int('units', min_value = 256, max_value = 512, step = 128)\n","  hp_lstm_units = hp.Int('lstm_units', min_value = 256, max_value = 512, step = 128)\n","  hp_embedding_dims = hp.Choice('embedding_dims', values = [300])\n","  hp_dropout = hp.Choice('dropout', values = [0.0, 0.1])\n","  hp_learning_rate = hp.Choice('learning_rate', values = [0.01, 0.001])\n","\n","  hypermodel = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(encoder.vocab_size + 1, hp_units),\n","    tf.keras.layers.Bidirectional(layers.LSTM(hp_lstm_units, return_sequences=True)),\n","    tf.keras.layers.Dropout(hp_dropout),\n","    tf.keras.layers.Bidirectional(layers.LSTM(hp_lstm_units, return_sequences=True)),\n","    tf.keras.layers.Dropout(hp_dropout),\n","    tf.keras.layers.Bidirectional(layers.LSTM(hp_lstm_units)),\n","    tf.keras.layers.Dropout(hp_dropout),\n","    tf.keras.layers.Dense(hp_units, activation=activations.relu),\n","    tf.keras.layers.Dense(hp_units, activation=activations.relu),\n","    tf.keras.layers.Dense(len(FILE_NAMES))\n","  ])\n","  hypermodel.compile(optimizer=optimizers.Adamax(learning_rate = hp_learning_rate),\n","              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","  \n","  return hypermodel"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2v8MXrJrwRmp"},"source":["### Run the tuner"]},{"cell_type":"markdown","metadata":{"id":"W9Gd2vHVRvNw"},"source":["The result are the optimal hyperparameters: `best_hps`."]},{"cell_type":"code","metadata":{"id":"PvrDPbJsMN7A"},"source":["class ClearTrainingOutput(tf.keras.callbacks.Callback):\n","  def on_train_end(*args, **kwargs):\n","    IPython.display.clear_output(wait = True)\n","\n","hyperband_tuner = kt.Hyperband(model_builder,\n","                     objective='val_accuracy', \n","                     max_epochs=35,\n","                     factor=3,\n","                     directory='/content/drive/My Drive/RUAK/output/hp_tuning', # This path may need to be changed.\n","                     project_name=f'Hyperband_{SESSION_ID}',\n","                     overwrite=True)\n","\n","hyperband_tuner.search(train_data, epochs =1, validation_data = test_data, callbacks = [ClearTrainingOutput()])\n","\n","best_hps_hyperband = hyperband_tuner.get_best_hyperparameters(1)[0]\n","\n","print(f\"\"\"\n","Optimal values:\n","- number of units in densely-connected layers {best_hps.get('units')}\n","- number of units in lstm {best_hps.get('lstm_units')}\n","- embedding dim {best_hps.get('embedding_dims')} \n","- learning rate {best_hps.get('learning_rate')}\n","- dropout rate {best_hps.get('dropout')}\n","\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lf2yDw2C3YbH"},"source":["hyperband_tuner.results_summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"98S5K1EJ33gL"},"source":["random_search_tuner = kt.RandomSearch(model_builder,\n","                        objective='val_accuracy',\n","                        max_trials=5, executions_per_trial=3,\n","                        directory='/content/drive/My Drive/RUAK/output/hp_tuning', # This path may need to be changed.\n","                        project_name=f'RandomSearch_{SESSION_ID}',\n","                        overwrite=True)\n","\n","random_search_tuner.search(train_data, epochs =2, validation_data = test_data, callbacks = [ClearTrainingOutput()])\n","\n","best_hps_random_search = random_search_tuner.get_best_hyperparameters(1)[0]\n","\n","print(f\"\"\"\n","Optimal values:\n","- number of units in densely-connected layers {best_hps.get('units')}\n","- number of units in lstm {best_hps.get('lstm_units')}\n","- embedding dim {best_hps.get('embedding_dims')} \n","- learning rate {best_hps.get('learning_rate')}\n","- dropout rate {best_hps.get('dropout')}\n","\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xk44Zn7x4ZJu"},"source":["random_search_tuner.results_summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6viAzXep-Wps"},"source":["# TensorBoard preparations"]},{"cell_type":"code","metadata":{"id":"Kh0T0yWPTJlw"},"source":["%load_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oX7suDrVRiYq"},"source":["log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%d.%m.%Y - %H:%M:%S\"))\n","\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u2PdigVLCdcL"},"source":["# Model preparation"]},{"cell_type":"markdown","metadata":{"id":"hz9f9MAqhg6c"},"source":["### Prepare callbacks"]},{"cell_type":"code","metadata":{"id":"mCZgMlv-hkaU"},"source":["checkpoint_path = f\"/content/drive/My Drive/RUAK/output/training_checkpoints/{SESSION_ID}/cp.ckpt\" # This path may need to be changed.\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","model_path = f\"/content/drive/My Drive/RUAK/output/models/model-{SESSION_ID}.h5\" # This path may need to be changed.\n","\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","                                                 save_weights_only=True,\n","                                                 verbose=1)\n","\n","class Callbacks(tf.keras.callbacks.Callback):\n","  def on_epoch_end(self, epoch, logs={}):\n","    if(logs.get('val_loss')<0.015):\n","      print(\"\\nTraining val. loss reached 0.015.\") # TODO: This should be an early stopping callback from Tensorflow.\n","      self.model.stop_training = True\n","\n","epoch_callbacks = Callbacks()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rfr9-X8qE94B"},"source":["### Set the hyperparameters"]},{"cell_type":"code","metadata":{"id":"TQm0U8ouC-G1"},"source":["EMBEDDING_DIMS = 700\n","NUM_LSTM_UNITS = 256\n","NUM_UNITS = 512\n","DROPOUT = 0.1\n","OUTPUT = len(FILE_NAMES)\n","LEARNING_RATE = 0.01\n","OPTIMIZER = optimizers.Adamax(learning_rate=LEARNING_RATE)\n","EPOCHS = 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RMIu_WjOFDFF"},"source":["### Setup and complie the model"]},{"cell_type":"markdown","metadata":{"id":"ayYJz2nRpQF9"},"source":["Load the Word_2_Vec model (700 dims, 100 epochs, window 7) for providing the weights for the embedding layer."]},{"cell_type":"code","metadata":{"id":"7Y9YgOClCcyD"},"source":["model_path = os.path.abspath(\"/content/drive/My Drive/RUAK/output/embedding/w2v/\") # This path may need to be changed.\n","\n","def get_embedding_matrix(model_name):\n","    model = Word2Vec.load(f'{model_path}/{model_name}')\n","    embedding_matrix = np.zeros((len(model.wv.vocab), model.vector_size))\n","    \n","    for i in range(len(model.wv.vocab)):\n","        embedding_vector = model.wv[model.wv.index2word[i]]\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","    print(f\"Embedding_matrix shape: {embedding_matrix.shape}\")\n","    return embedding_matrix\n","\n","embedding_matrix = get_embedding_matrix('full_700_iter100_win7_8.model') # Model is created by Word2Vec."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vknsVQuPpMA3"},"source":["Build and compile the model"]},{"cell_type":"code","metadata":{"id":"_Y8dl2ZYpHEP"},"source":["model = keras.Sequential([\n","    layers.Embedding(len(embedding_matrix), EMBEDDING_DIMS, weights=[\n","                     embedding_matrix], trainable=False),\n","    layers.Bidirectional(layers.LSTM(\n","        NUM_LSTM_UNITS, return_sequences=True)),\n","    layers.Dropout(DROPOUT),\n","    layers.Bidirectional(layers.LSTM(\n","        NUM_LSTM_UNITS, return_sequences=True)),\n","    layers.Dropout(DROPOUT),\n","    layers.Bidirectional(layers.LSTM(NUM_LSTM_UNITS)),\n","    layers.Dropout(DROPOUT),\n","    layers.Dense(NUM_UNITS, activation=activations.relu),\n","    layers.Dense(NUM_UNITS, activation=activations.relu),\n","    layers.Dense(OUTPUT, activation=activations.softmax)\n","])\n","\n","model.compile(optimizer=OPTIMIZER, loss=losses.SparseCategoricalCrossentropy(from_logits=True), \n","              metrics=['accuracy'])\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S4U6I-8lCivy"},"source":["# Model training"]},{"cell_type":"code","metadata":{"id":"6gksI7UJCce6"},"source":[" model.fit(train_data, epochs=EPOCHS, validation_data=test_data, callbacks=[epoch_callbacks, cp_callback])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AYJ4k-VLKHdo"},"source":["Save the model"]},{"cell_type":"code","metadata":{"id":"y4TgjSFvKG7c"},"source":["model.save('/content/drive/My Drive/RUAK/output/models/phil_model.h5') # This path may need to be changed."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KpOp6znCCCpu"},"source":["# Loading"]},{"cell_type":"markdown","metadata":{"id":"Dua2WMwGCHFd"},"source":["### Load model"]},{"cell_type":"code","metadata":{"id":"iAh4a9qiCFew"},"source":["model = tf.keras.models.load_model('/content/drive/My Drive/RUAK/output/models/phil_model.h5') # This path may need to be changed."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dMm4ri7S_XTn"},"source":["### Load stored weights"]},{"cell_type":"code","metadata":{"id":"vwyOI709_Tu6"},"source":["checkpoint_path = f\"/content/drive/My Drive/RUAK/training_checkpoints/{ID}/cp.ckpt\" # This path may need to be changed.\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","latest = tf.train.latest_checkpoint(checkpoint_dir)\n","model.load_weights(latest)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uHHVXU4C7nc9"},"source":["# Evaluate"]},{"cell_type":"code","metadata":{"id":"1yB-R-XK7rEx"},"source":["test_loss, test_acc = model.evaluate(test_data)\n","\n","print('Test Loss: {}'.format(test_loss))\n","print('Test Accuracy: {}'.format(test_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UAv-WNEmMxti"},"source":["Get index of author "]},{"cell_type":"code","metadata":{"id":"BpLozmZOGU7I"},"source":["sample_sentence_text = \"This is a test.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VhElQ-nKN7em"},"source":["import operator\n","\n","def pad_to_size(vec, size):\n","  zeros = [0] * (size - len(vec))\n","  vec.extend(zeros)\n","  return vec\n","\n","def sample_predict(sample_text, pad):\n","  encoded_sample_text = encoder.encode(sample_text)\n","\n","  if pad:\n","    encoded_sample_text = pad_to_size(encoded_sample_text, BATCH_SIZE)\n","\n","  encoded_sample_text = tf.cast(encoded_sample_text, tf.float32)\n","  predictions = model.predict(tf.expand_dims(encoded_sample_text, 0))\n","\n","  return (predictions)\n","\n","predictions_padding = sample_predict(sample_sentence_text, pad=True)\n","predictions = sample_predict(sample_sentence_text, pad=False)\n","\n","print('With padding:')\n","print(predictions_padding)\n","print('\\n')\n","print('Without padding:')\n","print(predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l5TqibMKjSmM"},"source":["# TensorBoard analysis"]},{"cell_type":"code","metadata":{"id":"8oJRvGKH7KmT"},"source":["%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YBv-S7ER6979"},"source":["!rm -rf ./logs/"],"execution_count":null,"outputs":[]}]}