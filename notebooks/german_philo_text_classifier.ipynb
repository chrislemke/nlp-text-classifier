{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"german_philo_text_classifier.ipynb","provenance":[{"file_id":"1-GLcGn8_wvQsyMD6VCOHgtZ_Zsw3eqsr","timestamp":1589599796682}],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kmrfYSe3pcBQ"},"source":["# 1. Preparations"]},{"cell_type":"code","metadata":{"id":"DmSWDUvClRyu"},"source":["!rm -rf ./logs/\n","!pip install tensorflow_datasets\n","!pip install -q -U keras-tuner\n","!pip install -q pyyaml h5py\n","!spacy download de_core_news_md"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jRrVEqH_HfJu"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g9eYykTQedMP"},"source":["import urllib, IPython, os, datetime, re, nltk, tensorboard, operator, json, requests\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout, BatchNormalization \n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.python.keras.callbacks import TensorBoard\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import tensorflow.keras.activations as activations\n","import tensorflow.keras.losses as losses\n","import tensorflow.keras.optimizers as optimizers\n","import kerastuner as kt\n","from keras.utils.vis_utils import plot_model\n","import matplotlib.pyplot as plt\n","from gensim.models import Word2Vec\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from wordcloud import WordCloud\n","from collections import Counter\n","import spacy\n","from spacy.lemmatizer import Lemmatizer\n","from spacy.lookups import Lookups\n","\n","%matplotlib inline\n","%load_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5jxRNdmNXj27"},"source":["**Google Cobalb runtime needs to be restarted before Spacy can be used!**"]},{"cell_type":"code","metadata":{"id":"h4h39FtOXg5Q"},"source":["os.kill(os.getpid(), 9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L3mTvrTLECOP"},"source":["Additional downloads"]},{"cell_type":"code","metadata":{"id":"6mjK0f0vEAqU"},"source":["nltk.download('punkt')\n","spacy.prefer_gpu()\n","nlp = spacy.load('de_core_news_md')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dN4kYwXzwGu-"},"source":["session_id = datetime.datetime.now().strftime(\"%d/%m/%Y-%H:%M\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FzJEXoSrfTJm"},"source":["Add the file names which should be processed. Files should be named after philosophers (e.g. `plato.txt`)."]},{"cell_type":"code","metadata":{"id":"NW35U-2oXscx"},"source":["file_names = [\n","    'kant.txt', \n","    'nietzsch.txt', \n","    'platon.txt', \n","    'rousseau.txt']\n","phil_names = [name[:-4].capitalize() for name in file_names]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y9yIECZO91cG"},"source":["# 2. Loading data and handle stop words"]},{"cell_type":"markdown","metadata":{"id":"x4TPXvl1gYS4"},"source":["## 2.1. Loading files with Keras"]},{"cell_type":"code","metadata":{"id":"tgTeoYVTRrw9"},"source":["prefix = 'file://'\n","processed_path = '/content/drive/My Drive/RUAK/input/processed/'\n","url = urllib.parse.quote(processed_path)\n","\n","for file_name in file_names:\n","  text_dir = tf.keras.utils.get_file(file_name, origin=prefix+url+file_name)\n","\n","parent_dir = os.path.dirname(text_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fBdj3KzTqJXz"},"source":["## 2.2 Load [stop words](https://en.wikipedia.org/wiki/Stop_word)"]},{"cell_type":"code","metadata":{"id":"kOifz08hn-KO","executionInfo":{"status":"ok","timestamp":1605293386194,"user_tz":-60,"elapsed":2859,"user":{"displayName":"Christopher Lemke","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg8xLBQIrtYOfid19YUJKEfwO5E7jg4_al1QsOe=s64","userId":"09454373583155263785"}},"outputId":"1227e955-f7e1-433d-e268-a6a6cca6727e","colab":{"base_uri":"https://localhost:8080/"}},"source":["def replace_umlaut(string):\n","    string = string.replace('ä', 'ae')\n","    string = string.replace('ö', 'oe')\n","    string = string.replace('ü', 'ue')\n","    string = string.replace('Ä', 'Ae')\n","    string = string.replace('Ö', 'Oe')\n","    string = string.replace('Ü', 'Ue')\n","    string = string.replace('ß', 'ss')\n","    return string\n","\n","response = json.loads(requests.get('https://countwordsfree.com/stopwords/german/json').text)\n","stop_words = set([replace_umlaut(word) for word in response['words']])\n","print(f'Loaded {len(stop_words)} stop words.')"],"execution_count":87,"outputs":[{"output_type":"stream","text":["Loaded 579 stop words.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VWMcoy4L0HHN"},"source":["Manually add stop words."]},{"cell_type":"code","metadata":{"id":"N9gd_wyx0Ee-","executionInfo":{"status":"ok","timestamp":1605293387727,"user_tz":-60,"elapsed":1138,"user":{"displayName":"Christopher Lemke","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg8xLBQIrtYOfid19YUJKEfwO5E7jg4_al1QsOe=s64","userId":"09454373583155263785"}},"outputId":"d965310c-5f1d-4548-b07a-ed063dfc8f54","colab":{"base_uri":"https://localhost:8080/"}},"source":["manual_stop_words = set(['naemlich', 'bloss'])\n","\n","stop_words = stop_words.union(manual_stop_words)\n","print(f'Total stop words count: {len(stop_words)}.')"],"execution_count":88,"outputs":[{"output_type":"stream","text":["Total stop words count: 581.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"96g2waSU9t-i"},"source":["# 3. Collect and organize data"]},{"cell_type":"markdown","metadata":{"id":"KeNOOlWpTu71"},"source":["Function to add words to `words_without_stop_words` and `unique_words_without_stop_words`."]},{"cell_type":"code","metadata":{"id":"R7vT1iazMBIU","executionInfo":{"status":"ok","timestamp":1605293392027,"user_tz":-60,"elapsed":1356,"user":{"displayName":"Christopher Lemke","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg8xLBQIrtYOfid19YUJKEfwO5E7jg4_al1QsOe=s64","userId":"09454373583155263785"}}},"source":["def add_words(sentence):\n","  for word in sentence.split():\n","    word = re.sub(r\"[^a-zA-Z]+\", \"\", word)\n","    if word == '':\n","      continue\n","    if word.lower() not in stop_words:\n","      words_without_stop_words.append(word)\n","      unique_words_without_stop_words.add(word)\n","\n","words_without_stop_words = []\n","unique_words_without_stop_words = set()"],"execution_count":90,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8xaLeKDYq5hX"},"source":["## 3.1. Extract sentences"]},{"cell_type":"markdown","metadata":{"id":"wGNi3Yh7mt3y"},"source":["Extract sentences from files and creates labels list. Adjust the language for the `nltk.sent_tokenizer` if needed."]},{"cell_type":"code","metadata":{"id":"gdnFXAtFdYrG","executionInfo":{"status":"error","timestamp":1605294110825,"user_tz":-60,"elapsed":711846,"user":{"displayName":"Christopher Lemke","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg8xLBQIrtYOfid19YUJKEfwO5E7jg4_al1QsOe=s64","userId":"09454373583155263785"}},"outputId":"45cbbc77-a5a7-4b5a-c5bf-b42c9b8c158d","colab":{"base_uri":"https://localhost:8080/","height":545}},"source":["labels = []\n","sentences = []\n","\n","for index, file_name in enumerate(file_names):\n","\n","  path = os.path.join(parent_dir, file_name)\n","\n","  with open(path, 'rb') as file: \n","    text = str(file.read())\n","    nltk_sentences = nltk.sent_tokenize(text, language='german')\n","\n","    for sentence in nltk_sentences:\n","      if ' ' in sentence == False:\n","        continue\n","      if len(sentence) <= 20:\n","        continue\n","      sentence = str(sentence).replace(\"b'\", \"\")\n","      sentences.append(sentence)\n","      labels.append(index)\n","      add_words(sentence)\n","\n","    print(f\"Sentences for {file_name} with label: {index} added.\")\n","\n","print(f'\\n{df.shape[0]} sentences found.')\n","print(f'{len(words_without_stop_words)} words found (excl. stop words).')\n","print(f'{len(unique_words_without_stop_words)} unique words found (excl. stop words).')"],"execution_count":91,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-91-d44ef731517b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0madd_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Sentences for {file_name} with label: {index} added.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-90-07d0927fab13>\u001b[0m in \u001b[0;36madd_words\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0mwords_without_stop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0munique_words_without_stop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-89-c48a22f20a50>\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'VERB'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.__call__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.predict\u001b[0;34m()\u001b[0m\n","\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserModel.begin_update\u001b[0;34m()\u001b[0m\n","\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/api.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(seqs_in, drop)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbp_layer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/resnet.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbp_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/layernorm.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprop_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mbackprop_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/maxout.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X__bi, drop)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdrop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mdrop\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0moutput__boc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgemm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX__bi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0moutput__boc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnO\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0moutput__boc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput__boc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput__boc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"muvXsp-L2RjH"},"source":["## 3.2. Create DataFrame and extend data"]},{"cell_type":"markdown","metadata":{"id":"awuDoZibMw6p"},"source":["Collect most commen words except the stop words."]},{"cell_type":"code","metadata":{"id":"9CINAlwkMf4M","executionInfo":{"status":"aborted","timestamp":1605294110823,"user_tz":-60,"elapsed":703941,"user":{"displayName":"Christopher Lemke","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg8xLBQIrtYOfid19YUJKEfwO5E7jg4_al1QsOe=s64","userId":"09454373583155263785"}}},"source":["most_common = [word[0] for word in Counter(words_without_stop_words).most_common(20)]\n","most_common_count = mydict = {k: v for k, v in Counter(words_without_stop_words).most_common(20)}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fxUWXnD72gHL"},"source":["Some helper methods"]},{"cell_type":"code","metadata":{"id":"10sfumjtrcsw"},"source":["def stop_word_quote_fn(sentence):\n","  count = 0\n","  for word in sentence.split():\n","    word = re.sub(r\"[^a-zA-Z]+\", \"\", word)\n","    if word.lower() in stop_words:\n","      count += 1\n","  return round(count/len(sentence.split()) * 100, 2)\n","\n","def stop_word_count_fn(sentence):\n","  count = 0\n","  for word in sentence.split():\n","    word = re.sub(r\"[^a-zA-Z]+\", \"\", word)\n","    if word.lower() in stop_words:\n","      count += 1\n","  return count\n","\n","def mean_word_length_fn(sentence):\n","  return round(np.array([len(word) for word in sentence.replace('.','').split()]).mean(), 2)\n","\n","def pos_count(sentence, pos):\n","  doc = nlp(sentence)\n","  return len([w.pos_ for w in doc if w.pos_ == pos])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZxICrBEKbZE1"},"source":["### 3.2.1. Create DataFrame"]},{"cell_type":"code","metadata":{"id":"uFs6VfqrbQzg"},"source":["df = pd.DataFrame({'label': labels, 'sentence': sentences})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XtHcxdysQuPe"},"source":["### 3.2.2. Construct new data"]},{"cell_type":"code","metadata":{"id":"qql42ncCQtK5"},"source":["df['philosopher'] = df['label'].map(lambda x: phil_names[x])\n","df['word_count'] = df['sentence'].str.split().str.len()\n","df['mean_word_length'] = df['sentence'].map(mean_word_length_fn)\n","df['stop_words_quote'] = df['sentence'].map(stop_word_quote_fn)\n","df['stop_words_count'] = df['sentence'].map(stop_word_count_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j-PMNxYGUHj_"},"source":["Add columns and values for [POS tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging). Annotations can be found [here](https://spacy.io/api/annotation). **This may take a while!**"]},{"cell_type":"code","metadata":{"id":"oaAHSANZUG9J"},"source":["pos_tags = ['ADJ', 'ADV', 'ADP', 'AUX', 'DET', 'NUM', 'X', 'INTJ', 'CCONJ', 'SCONJ', 'PROPN', 'NOUN', 'DET', 'PRON', 'PART', 'VERB']\n","for pos in pos_tags:\n","  df[f'{pos}_count'] = df['sentence'].map(lambda sen: pos_count(sen, pos))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4JYzLWqkrh_Y"},"source":["### 3.2.3. Preview DataFrame"]},{"cell_type":"code","metadata":{"id":"7mXU1V4GazIh"},"source":["df.head(len(sentences))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1t6t2UGRo1Jg"},"source":["# 4. Store and load a DataFrame"]},{"cell_type":"markdown","metadata":{"id":"PYrSJXlMn5Q_"},"source":["Save DataFrame to CSV if needed."]},{"cell_type":"code","metadata":{"id":"yUD-BM87n9tJ"},"source":["df.to_csv('/content/drive/My Drive/RUAK/output/dataframe/dataframe.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FtFcnhfko9wQ"},"source":["Load the DataFrame from CSV if needed."]},{"cell_type":"code","metadata":{"id":"naVqKAOXpDoL"},"source":["df = pd.read_csv('/content/drive/My Drive/RUAK/output/dataframe/dataframe.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Fo0XiX7S6dr"},"source":["# 5. Visualization of data"]},{"cell_type":"markdown","metadata":{"id":"DkQFhI6J0Sw_"},"source":["## 5.1 Prepare values for visualization"]},{"cell_type":"markdown","metadata":{"id":"63Nrf9EI1ugk"},"source":["[Lemmatize](https://en.wikipedia.org/wiki/Lemmatisation)"]},{"cell_type":"code","metadata":{"id":"COZcha201wQP"},"source":["def lemmatize(word):\n","  doc = nlp(word)\n","  for word in doc:\n","    if word.pos_ == 'VERB':\n","      return word.lemma_.replace(' ', '') \n","    return word.text.replace(' ', '')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dg-2j4xO_Ua7"},"source":["Count vocabulary"]},{"cell_type":"code","metadata":{"id":"GpYhWH3Njg0h"},"source":["def vocabulary_count_fn(series):\n","  vocabulary = set()\n","  for sentence in series:\n","    words = sentence.split()\n","    for word in words:\n","      if word.lower() not in stop_words:\n","        word = re.sub(r\"[^a-zA-Z]+\", \"\", word)\n","        word = lemmatize(word)\n","        vocabulary.add(word.lower())\n","  return len(vocabulary)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CH14xXHg_i0c"},"source":["Values from DataFrame"]},{"cell_type":"code","metadata":{"id":"XieassmAz9fb"},"source":["median_sentence_length = df.groupby('philosopher')['word_count'].median()\n","mean_stop_words = df.groupby('philosopher')['stop_words_quote'].mean()\n","sentence_count = df.groupby('philosopher')['sentence'].count()\n","unique_vocabulary_count = df.groupby('philosopher')['sentence'].apply(lambda ser: vocabulary_count_fn(ser))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z3e6981g09wl"},"source":["## 5.2. Draw visualization"]},{"cell_type":"markdown","metadata":{"id":"6TRUVgAr_7j9"},"source":["### 5.2.1. Comparing philosophers"]},{"cell_type":"code","metadata":{"id":"oJkpwQ6IUyKD"},"source":["fig, axs = plt.subplots(4,1, figsize=(10,15))\n","\n","fig.tight_layout(h_pad=6)\n","\n","axs[0].bar(phil_names, sentence_count)\n","axs[0].set_ylabel('Number of sentences', fontdict={'color':'gray', 'size':12})\n","axs[0].tick_params(axis='both', colors='gray', labelsize=12)\n","axs[0].grid()\n","\n","axs[1].bar(phil_names, median_sentence_length)\n","axs[1].set_ylabel('Median sentence lenth', fontdict={'color':'gray', 'size':12})\n","axs[1].tick_params(axis='both', colors='gray', labelsize=12)\n","axs[1].grid()\n","\n","axs[2].bar(phil_names, unique_vocabulary_count)\n","axs[2].set_ylabel('Unique vocabulary count\\n(excl. stop words)', fontdict={'color':'gray', 'size':12})\n","axs[2].tick_params(axis='both', colors='gray', labelsize=12)\n","axs[2].grid()\n","\n","axs[3].bar(phil_names, mean_stop_words)\n","axs[3].set_ylabel('Median stop words quote', fontdict={'color':'gray', 'size':12})\n","axs[3].tick_params(axis='both', colors='gray', labelsize=12)\n","axs[3].grid()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QdmHHaPJ_zdx"},"source":["### 5.2.2. Common words in sentences"]},{"cell_type":"code","metadata":{"id":"yUqVaJBpR8mW"},"source":["fig, axs = plt.subplots(1,1, figsize=(30,10))\n","\n","axs.bar(most_common_count.keys(), most_common_count.values(), color='orange')\n","axs.set_ylabel('Number of sentences', fontdict={'color':'gray', 'size':12})\n","axs.tick_params(axis='both', colors='gray', labelsize=12)\n","axs.grid()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8YEfw66S_oVo"},"source":["### 5.2.3. Word cloud"]},{"cell_type":"code","metadata":{"id":"QmPbGhTn5tHy"},"source":["wordcloud = WordCloud(width=5000, \n","                      height=4000,\n","                      max_words=20,  \n","                      background_color ='black', \n","                      stopwords = stop_words, \n","                      min_font_size = 10).generate_from_frequencies(most_common_count) \n","\n","plt.figure(figsize=(20, 12), facecolor='k', edgecolor ='k') \n","plt.imshow(wordcloud) \n","plt.axis(\"off\") \n","plt.tight_layout(pad=0) \n","plt.show() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"61D3POnhr3fT"},"source":["# 6. Prepare text data"]},{"cell_type":"markdown","metadata":{"id":"Aad2fR1Rs5bn"},"source":["## 6.1. Tokenize"]},{"cell_type":"code","metadata":{"id":"TKA1Om5OtNFd"},"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(df['sentence'].values)\n","print(f\"{len(df['sentence'].values)} sentences from {len(file_names)} authors.\")\n","print(f'{len(tokenizer.word_counts)} unique vocabularies.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JwIYTz819jIk"},"source":["## 6.2. Encode"]},{"cell_type":"code","metadata":{"id":"DaLmBJfP73gJ"},"source":["encoded_sentences = tokenizer.texts_to_sequences(df['sentence'].values)\n","padded_sentences = pad_sequences(encoded_sentences, padding='post')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s5mvazkEy4pF"},"source":["Test the encoder"]},{"cell_type":"code","metadata":{"id":"ZeS-BVU9y6d7"},"source":["print(df['sentence'].values[7])\n","print(np.array(padded_sentences[7]))\n","print(tokenizer.sequences_to_texts([padded_sentences[7]]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dZMGHboj8yRE"},"source":["## 6.3. Splitting"]},{"cell_type":"markdown","metadata":{"id":"1nZjfiMr-QLV"},"source":["Create train and test data for the fitting proccess."]},{"cell_type":"code","metadata":{"id":"5uHhk9JCoyXD"},"source":["X_train, X_valid, y_train, y_valid = train_test_split(padded_sentences, df['label'].values, test_size=0.1)\n","print(X_train.shape)\n","print(y_train.shape)\n","print(X_valid.shape)\n","print(y_valid.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PVVYN4t5kJU2"},"source":["# 7. Hyperparameter tuning"]},{"cell_type":"markdown","metadata":{"id":"xur6_r2MJ2QE"},"source":["## 7.1. Setup the hypermodel"]},{"cell_type":"markdown","metadata":{"id":"ayYJz2nRpQF9"},"source":["Load the Word2Vec model (700 dims, 100 epochs, window 7) for providing the weights for the embedding layer."]},{"cell_type":"code","metadata":{"id":"7Y9YgOClCcyD"},"source":["model_path = os.path.abspath(\"/content/drive/My Drive/RUAK/output/embedding/w2v/\") # This path may need to be changed.\n","\n","def get_embedding_matrix(model_name):\n","    model = Word2Vec.load(f'{model_path}/{model_name}')\n","    embedding_matrix = np.zeros((len(model.wv.vocab), model.vector_size))\n","    \n","    for i in range(len(model.wv.vocab)):\n","        embedding_vector = model.wv[model.wv.index2word[i]]\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","    print(f\"Embedding_matrix shape: {embedding_matrix.shape}\")\n","    return embedding_matrix\n","\n","embedding_matrix = get_embedding_matrix('full_700_iter100_win7_8.model')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZU_5ktAbtXUz"},"source":["Define the model"]},{"cell_type":"code","metadata":{"id":"i5uA91DTI6I0"},"source":["def hypermodel(hp):\n","\n","  hp_dense_count = hp.Int('dense_count', min_value=1, max_value=7, step=1)\n","  hp_embedding_trainable = hp.Choice('embedding_trainable', [True, False])\n","  hp_with_batch_normalization = hp.Choice('with_batch_normalization', [True, False])\n","  hp_lstm_units = hp.Int('lstm_units', 256, 512, step=128)\n","  hp_dropout = hp.Choice('dropout', [0.0, 0.1, 0.25])\n","  hp_learning_rate = hp.Choice('learning_rate', [0.01, 0.001, 0.0001])\n","  hp_adam_epsilon = hp.Choice('adam_epsilon', values=[1e-07, 1e-08])\n","\n","  model = tf.keras.Sequential()\n","                                    \n","  model.add(Embedding(len(embedding_matrix),\n","                    output_dim=700,\n","                    weights=[embedding_matrix], \n","                    trainable=hp_embedding_trainable,\n","                    mask_zero=True))\n","  \n","  model.add(Bidirectional(LSTM(hp_lstm_units, return_sequences=True)))\n","  if hp_embedding_trainable == True:\n","    BatchNormalization()\n","  model.add(Dropout(hp_dropout))\n","\n","  model.add(Bidirectional(LSTM(hp_lstm_units, return_sequences=True)))\n","  if hp_embedding_trainable == True:\n","    BatchNormalization()  \n","  model.add(Dropout(hp_dropout))\n","\n","  model.add(Bidirectional(LSTM(hp_lstm_units)))\n","  if hp_embedding_trainable == True:\n","    BatchNormalization()  \n","  model.add(Dropout(hp_dropout))\n","\n","  for i in range(hp_dense_count):\n","    hp_dense_units = hp.Int(f'dense_units{i}', 64, 512, step=64)\n","    hp_dense_activation = hp.Choice(f'dense_activation_{i}', values=['tanh', 'relu'])\n","    model.add(Dense(hp_dense_units, activation=hp_dense_activation))\n","\n","  model.add(Dense(len(file_names), activation='softmax'))\n","\n","  model.compile(optimizer=optimizers.Adam(learning_rate=hp_learning_rate, epsilon=hp_adam_epsilon),\n","              loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2v8MXrJrwRmp"},"source":["## 7.2. Run the tuner"]},{"cell_type":"markdown","metadata":{"id":"ODaPNlZl4txb"},"source":["Some parameters needed for tuning and training."]},{"cell_type":"code","metadata":{"id":"s61Szf7jzHLh"},"source":["batch_size=40\n","epochs=1\n","search_epochs=1\n","early_stopping_patience=5\n","executions_per_trial=1\n","hyperband_iterations=1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W9Gd2vHVRvNw"},"source":["The result are the optimal hyperparameters: `best_hps`."]},{"cell_type":"code","metadata":{"id":"PvrDPbJsMN7A"},"source":["class ClearTrainingOutput(tf.keras.callbacks.Callback):\n","  def on_train_end(*args, **kwargs):\n","    IPython.display.clear_output(wait=True)\n","\n","tuner = kt.Hyperband(hypermodel,\n","                     objective='val_accuracy', \n","                     executions_per_trial=executions_per_trial,\n","                     factor=3,\n","                     max_epochs=epochs+5,\n","                     hyperband_iterations=hyperband_iterations,\n","                     directory='/content/drive/My Drive/RUAK/output/hp_tuning', # This path may need to be changed.\n","                     project_name='RUAK',\n","                     overwrite=True)\n","\n","tuner.search(X_train, y_train, \n","             epochs=search_epochs,\n","             validation_data = (X_valid, y_valid),\n","             callbacks = [ClearTrainingOutput(), EarlyStopping('val_accuracy', patience=1)],\n","             verbose=2)\n","\n","best_hps = tuner.get_best_hyperparameters(1)[0]\n","best_models = tuner.get_best_models(num_models=3)\n","\n","print(f\"\"\"\n","Optimal values:\n","- embedding is trainable {best_hps.get('embedding_trainable')}\n","- number of units for dense layers {best_hps.get('dense_units')}\n","- number of units for lstm layers {best_hps.get('lstm_units')}\n","- learning rate {best_hps.get('learning_rate')}\n","- dropout rate {best_hps.get('dropout')}\n","\"\"\")\n","\n","tuner.results_summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u2PdigVLCdcL"},"source":["# 8. Model preparation"]},{"cell_type":"markdown","metadata":{"id":"T0RQAk0PbU_X"},"source":["Get summaries of the best models and choose the model for training."]},{"cell_type":"code","metadata":{"id":"PwVKYs2XzpUp"},"source":["for model in best_models:\n","  model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gyx6tUlnzK1M"},"source":["Choose preferred model"]},{"cell_type":"code","metadata":{"id":"zUMY2E68bUZ-"},"source":["chosen_model = best_models[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-_Tur-i_y5fB"},"source":["Plot model structure"]},{"cell_type":"code","metadata":{"id":"_c0wvzcRy4xO"},"source":["plot_model(chosen_model, show_shapes=True, show_layer_names=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hz9f9MAqhg6c"},"source":["## 8.1. Prepare callbacks"]},{"cell_type":"markdown","metadata":{"id":"2V0dYziFuKSK"},"source":["TensorBoard preparations"]},{"cell_type":"code","metadata":{"id":"oX7suDrVRiYq"},"source":["log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%d/%m/%Y - %H:%M\"))\n","tb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_E6Aba9xuinl"},"source":["Prepare `ModelCheckpoint` and `EarlyStopping` callbacks."]},{"cell_type":"code","metadata":{"id":"mCZgMlv-hkaU"},"source":["checkpoint_path = f\"/content/drive/My Drive/RUAK/output/training_checkpoints/{session_id}/cp.ckpt\" # This path may need to be changed.\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","model_path = f\"/content/drive/My Drive/RUAK/output/models/model-{session_id}.h5\" # This path may need to be changed.\n","\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","                                                 save_weights_only=True,\n","                                                 verbose=1)\n","\n","es_callback = EarlyStopping('val_accuracy', patience=early_stopping_patience, restore_best_weights=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S4U6I-8lCivy"},"source":["## 8.2. Model training"]},{"cell_type":"code","metadata":{"id":"6gksI7UJCce6"},"source":[" history = chosen_model.fit(X_train, \n","                            y_train, \n","                            epochs=epochs, \n","                            batch_size=batch_size, \n","                            validation_data=(X_valid, y_valid), \n","                            callbacks=[cp_callback, es_callback, tb_callback])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KpOp6znCCCpu"},"source":["# 9. Save or load the model"]},{"cell_type":"markdown","metadata":{"id":"AYJ4k-VLKHdo"},"source":["## 9.1. Save the model"]},{"cell_type":"code","metadata":{"id":"y4TgjSFvKG7c"},"source":["model.save('/content/drive/My Drive/RUAK/output/models/phil_model.h5') # This path may need to be changed."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dua2WMwGCHFd"},"source":["## 9.2. Load the model"]},{"cell_type":"code","metadata":{"id":"iAh4a9qiCFew"},"source":["chosen_model = tf.keras.models.load_model('/content/drive/My Drive/RUAK/output/models/phil_model.h5') # This path may need to be changed."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4jQajX37z3bO"},"source":["## 9.3. Load stored weights"]},{"cell_type":"markdown","metadata":{"id":"EfC2I1qfvf9w"},"source":["`{ID}` needs to be provided. Should be some kind of `session_id`."]},{"cell_type":"code","metadata":{"id":"7tzH24Hw0KQQ"},"source":["checkpoint_id = # Place the `session_id`"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vwyOI709_Tu6"},"source":["checkpoint_path = f\"/content/drive/My Drive/RUAK/training_checkpoints/{checkpoint_id}/cp.ckpt\" # This path may need to be changed.\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","latest = tf.train.latest_checkpoint(checkpoint_dir)\n","chosen_model.load_weights(latest)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uHHVXU4C7nc9"},"source":["# 10. Evaluation"]},{"cell_type":"markdown","metadata":{"id":"9UwQPCjCvscA"},"source":["## 10.1. Show loss and accuracy"]},{"cell_type":"code","metadata":{"id":"1yB-R-XK7rEx"},"source":["test_loss, test_acc = chosen_model.evaluate(valid_data)\n","\n","print('Test Loss: {}'.format(test_loss))\n","print('Test Accuracy: {}'.format(test_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RiHTWqlnv3Wi"},"source":["## 10.2. Test the model"]},{"cell_type":"markdown","metadata":{"id":"UAv-WNEmMxti"},"source":["Test the model. Add a `sample_sentence` to get the probability distribution for each author."]},{"cell_type":"code","metadata":{"id":"BpLozmZOGU7I"},"source":["sample_sentence = \"Die Vernunft ist staerker als der Wille.\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VhElQ-nKN7em"},"source":["def pad_to_size(vec, size):\n","  zeros = [0] * (size - len(vec))\n","  vec.extend(zeros)\n","  return vec\n","\n","def sample_predict(sample_text, pad):\n","  encoded_sample_text = encoder.encode(sample_text)\n","  if pad:\n","    encoded_sample_text = pad_to_size(encoded_sample_text, batch_size)\n","  encoded_sample_text = tf.cast(encoded_sample_text, tf.float32)\n","  predictions = model.predict(tf.expand_dims(encoded_sample_text, 0))\n","  return (predictions)\n","\n","predictions = sample_predict(sample_sentence_text, pad=True)\n","\n","print(predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l5TqibMKjSmM"},"source":["# 11. TensorBoard analysis"]},{"cell_type":"code","metadata":{"id":"8oJRvGKH7KmT"},"source":["%tensorboard --logdir logs"],"execution_count":null,"outputs":[]}]}