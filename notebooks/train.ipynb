{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":["7mlq0lHa3ceQ"],"mount_file_id":"1lppvohK2TyjT3MPdRWJflMHmNt3YgEsW","authorship_tag":"ABX9TyPRJTmdQiZbggeZdTNMVfnK"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gDQfErWuyo9E","colab_type":"text"},"source":["# Setup"]},{"cell_type":"code","metadata":{"id":"_tp7tyjS24Av","colab_type":"code","colab":{}},"source":["import nltk\n","import ast\n","nltk.download('punkt')\n","\n","INPUT_FILE_PATH = '/content/drive/My Drive/RUAK/input/processed'\n","TOKENIZED_PATH = '/content/drive/My Drive/RUAK/output/tokenized'\n","\n","INPUT_FILE = 'pg_kant.txt'\n","DOC2VEC_INPUT_FILES = ['pg_kant.txt']\n","\n","MODEL_PREFIX = 'xxx'\n","DOC2VEC_PREFIX = 'kant_platon'\n","SIZE = 300\n","EPOCHS = 30\n","WINDOW = 30\n","IDENTIFIER_NO = 14\n","MIN_COUNT = 10"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7mlq0lHa3ceQ","colab_type":"text"},"source":["# Word embedding"]},{"cell_type":"markdown","metadata":{"id":"yCdmFWnvx3hQ","colab_type":"text"},"source":["### Tokenize"]},{"cell_type":"code","metadata":{"id":"DVxOFsJT2P6_","colab_type":"code","colab":{}},"source":["with open(f'{INPUT_FILE_PATH}/{INPUT_FILE}', encoding='UTF-8') as file:\n","  sentences = nltk.sent_tokenize(file.read(), language='german')\n","  tokenized_text = []\n","  for sentence in sentences:\n","    if ' ' in sentence == False:\n","      continue\n","    if len(sentence) <= 20:\n","      continue\n","    tokenized_text.append(nltk.word_tokenize(sentence, language='german'))\n","  print(f'Created {len(tokenized_text)} tokens.')\n","  print('Preview:')\n","  print(tokenized_text[1])\n","\n","file_name = INPUT_FILE.replace('.txt', '')\n","with open(f'{TOKENIZED_PATH}/_{file_name}_tokenized.txt', 'w') as outfile:\n","  for entry in tokenized_text:\n","    outfile.write(''.join(str(entry)) + '\\n')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iIcuB3hV4JX0","colab_type":"text"},"source":["## FastText"]},{"cell_type":"markdown","metadata":{"id":"X7Cq2DtQx98X","colab_type":"text"},"source":["### Load tokens and build vocabulary"]},{"cell_type":"code","metadata":{"id":"8FSzNqArrgIG","colab_type":"code","colab":{}},"source":["from gensim.models import FastText\n","\n","file_name = INPUT_FILE.replace('.txt', '')\n","\n","loaded_tokenized_text = []\n","with open(f'{TOKENIZED_PATH}/_{file_name}_tokenized.txt', 'r') as infile:\n","  for line in infile:\n","    line = ast.literal_eval(line)\n","    loaded_tokenized_text.append(line)\n","\n","print(f'Loaded from file: {loaded_tokenized_text[:2]} ...')\n","\n","model = FastText(size=SIZE, window=WINDOW, min_count=MIN_COUNT)\n","model.build_vocab(sentences=loaded_tokenized_text)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bdjVoC_cyF0q","colab_type":"text"},"source":["### Train"]},{"cell_type":"code","metadata":{"id":"6eyv60nH6xSh","colab_type":"code","colab":{}},"source":["model.train(sentences=loaded_tokenized_text, total_examples=len(loaded_tokenized_text), epochs=EPOCHS)\n","\n","ft_model_output_path = '/content/drive/My Drive/RUAK/output/embedding/ft'\n","file_name = f'{MODEL_PREFIX}_{SIZE}_iter{EPOCHS}_win{WINDOW}_{IDENTIFIER_NO}-FT.model'\n","model.save(f'{ft_model_output_path}/{file_name}')\n","model.wv.save_word2vec_format(f'{ft_model_output_path}/{file_name}-bin.kv', binary=True)\n","model.wv.save_word2vec_format(f'{ft_model_output_path}/{file_name}-txt.kv', binary=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DvPflEECMNJb","colab_type":"text"},"source":["## Word2Vec"]},{"cell_type":"markdown","metadata":{"id":"sqAVCCGcODJY","colab_type":"text"},"source":["### Load tokens"]},{"cell_type":"code","metadata":{"id":"IYTAaATEMRbI","colab_type":"code","colab":{}},"source":["file_name = INPUT_FILE.replace('.txt', '')\n","\n","loaded_tokenized_text = []\n","with open(f'{TOKENIZED_PATH}/_{file_name}_tokenized.txt', 'r') as infile:\n","  for line in infile:\n","    line = ast.literal_eval(line)\n","    loaded_tokenized_text.append(line)\n","\n","print(f'Loaded from file: {loaded_tokenized_text[:2]} ...')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oMXUr_TdOH3-","colab_type":"text"},"source":["### Train"]},{"cell_type":"code","metadata":{"id":"IVCdQ9t3NmeT","colab_type":"code","colab":{}},"source":["from gensim.models import Word2Vec\n","import multiprocessing\n","\n","model = Word2Vec(loaded_tokenized_text, size=SIZE, window=WINDOW, min_count=MIN_COUNT, workers=multiprocessing.cpu_count())\n","\n","w2v_model_output_path = '/content/drive/My Drive/RUAK/output/embedding/w2v'\n","file_name = f'{MODEL_PREFIX}_{SIZE}_iter{EPOCHS}_win{WINDOW}_{IDENTIFIER_NO}-W2V.model'\n","model.save(f'{w2v_model_output_path}/{file_name}')\n","model.wv.save_word2vec_format(f'{w2v_model_output_path}/{file_name}-bin.kv', binary=True)\n","model.wv.save_word2vec_format(f'{w2v_model_output_path}/{file_name}-txt.kv', binary=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wV8yJpbk2xTw","colab_type":"text"},"source":["# Doc2Vec"]},{"cell_type":"markdown","metadata":{"id":"uz6gSgGmLb8k","colab_type":"text"},"source":["## Sentences"]},{"cell_type":"markdown","metadata":{"id":"WuojDYmW4Bgc","colab_type":"text"},"source":["### Prepare sentences"]},{"cell_type":"code","metadata":{"id":"XnYv28s420HT","colab_type":"code","colab":{}},"source":["from gensim.models.doc2vec import TaggedDocument\n","import multiprocessing\n","import pickle\n","\n","documents = []\n","for index, file_name in enumerate(DOC2VEC_INPUT_FILES):\n","  with open(f'{INPUT_FILE_PATH}/{file_name}', encoding='UTF-8') as file:\n","    sentences = nltk.sent_tokenize(file.read(), language='german')\n","    for sentence in sentences:\n","\n","      if ' ' in sentence == False:\n","        continue\n","      if len(sentence) <= 20:\n","        continue\n","      if sentence[0] == '-':\n","        sentence = sentence[1:]\n","\n","      tagged_document = TaggedDocument(nltk.word_tokenize(sentence, language='german'), [index])\n","      documents.append(tagged_document)\n","\n","print(f'Found {len(documents)} sentences in {len(DOC2VEC_INPUT_FILES)} files.')\n","print('Preview:')\n","print(documents[:len(DOC2VEC_INPUT_FILES)])\n","\n","with open(f'{TOKENIZED_PATH}/_{DOC2VEC_PREFIX}_doc2vec_sentence_tagged', 'wb') as outfile:\n","  pickle.dump(documents, outfile)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QdgSr-OBA98o","colab_type":"text"},"source":["### Load"]},{"cell_type":"code","metadata":{"id":"xLnReSJzBBR9","colab_type":"code","colab":{}},"source":["import pickle\n","\n","loaded_documents = []\n","with open(f'{TOKENIZED_PATH}/_{DOC2VEC_PREFIX}_doc2vec_sentence_tagged', 'rb') as infile:\n","  loaded_documents = pickle.load(infile)\n","\n","print(f'Loaded from file: {loaded_documents[:len(DOC2VEC_INPUT_FILES)]} ...')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vFZpmoSpLiHR","colab_type":"text"},"source":["## Documents"]},{"cell_type":"markdown","metadata":{"id":"mCsJ7dfeLwwr","colab_type":"text"},"source":["### Prepare Documents"]},{"cell_type":"code","metadata":{"id":"kOgXnDXWLzFr","colab_type":"code","colab":{}},"source":["from gensim.models.doc2vec import TaggedDocument\n","import multiprocessing\n","import pickle\n","\n","documents = []\n","for index, file_name in enumerate(DOC2VEC_INPUT_FILES):\n","  with open(f'{INPUT_FILE_PATH}/{file_name}', encoding='UTF-8') as file:\n","      document =  file.read()\n","      tagged_document = TaggedDocument(nltk.word_tokenize(document, language='german'), [index])\n","      documents.append(tagged_document)\n","\n","with open(f'{TOKENIZED_PATH}/_{DOC2VEC_PREFIX}_doc2vec_doc_tagged', 'wb') as outfile:\n","  pickle.dump(documents, outfile)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WCSmIT97MgEI","colab_type":"text"},"source":["### Load"]},{"cell_type":"code","metadata":{"id":"LTQ7SwI5MhVu","colab_type":"code","colab":{}},"source":["import pickle\n","\n","loaded_documents = []\n","with open(f'{TOKENIZED_PATH}/_{DOC2VEC_PREFIX}_doc2vec_doc_tagged', 'rb') as infile:\n","  loaded_documents = pickle.load(infile)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n1Cq-Qo7EHd-","colab_type":"text"},"source":["## Train"]},{"cell_type":"code","metadata":{"id":"MQp2F6mjDxQt","colab_type":"code","colab":{}},"source":["from gensim.models.doc2vec import Doc2Vec\n","import multiprocessing\n","\n","model = Doc2Vec(loaded_documents, vector_size=SIZE, window=WINDOW, min_count=MIN_COUNT, workers=multiprocessing.cpu_count())\n","\n","d2v_model_output_path = '/content/drive/My Drive/RUAK/output/embedding/d2v'\n","file_name = f'{DOC2VEC_PREFIX}_{SIZE}_iter{EPOCHS}_win{WINDOW}_{IDENTIFIER_NO}-D2V.model'\n","model.save(f'{d2v_model_output_path}/{file_name}')\n","model.docvecs.save_word2vec_format(f'{d2v_model_output_path}/{file_name}-bin.kv', binary=True)\n","model.docvecs.save_word2vec_format(f'{d2v_model_output_path}/{file_name}-txt.kv', binary=False)"],"execution_count":0,"outputs":[]}]}